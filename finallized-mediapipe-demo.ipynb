{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f6be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db34b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable some of the tf/keras training warnings \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "# suppress untraced functions warning\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95f2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained pose estimation model from Google Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Supported Mediapipe visualization tools\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69263cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from video frames\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d662002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7d7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ee756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(video_path):\n",
    "    \"\"\"\n",
    "    Get video properties like FPS, width, height, and frame count\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    return fps, width, height, total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784bf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_to_keypoints(video_path, target_sequence_length, pose_model):\n",
    "    \"\"\"\n",
    "    Process a video file and extract keypoints for each frame\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    keypoints_sequence = []\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate frame sampling if video is longer than target sequence\n",
    "    if total_frames > target_sequence_length:\n",
    "        # Sample frames evenly across the video\n",
    "        frame_indices = np.linspace(0, total_frames - 1, target_sequence_length, dtype=int)\n",
    "    else:\n",
    "        # Use all frames and pad if necessary\n",
    "        frame_indices = list(range(total_frames))\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Only process frames we want to sample\n",
    "        if frame_count in frame_indices:\n",
    "            # Make detection\n",
    "            image, results = mediapipe_detection(frame, pose_model)\n",
    "            \n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            keypoints_sequence.append(keypoints)\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Break if we've processed enough frames\n",
    "        if len(keypoints_sequence) >= target_sequence_length:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad sequence if it's shorter than target length\n",
    "    while len(keypoints_sequence) < target_sequence_length:\n",
    "        # Repeat the last frame or use zeros\n",
    "        if len(keypoints_sequence) > 0:\n",
    "            keypoints_sequence.append(keypoints_sequence[-1])\n",
    "        else:\n",
    "            keypoints_sequence.append(np.zeros(33*4))\n",
    "    \n",
    "    return np.array(keypoints_sequence[:target_sequence_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6af1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FOLDER_PATH = r\"C:\\Users\\pheno\\Downloads\\work\\excercises\" \n",
    "DATA_PATH = os.path.join(os.getcwd(), 'data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8aec5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory if it does not exist yet\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8861e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_EXTENSIONS = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aff050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found exercise classes: ['curl' 'lunge' 'plank' 'situp' 'squat']\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "if os.path.exists(VIDEO_FOLDER_PATH):\n",
    "    actions = [folder for folder in os.listdir(VIDEO_FOLDER_PATH) \n",
    "               if os.path.isdir(os.path.join(VIDEO_FOLDER_PATH, folder))]\n",
    "    actions = np.array(sorted(actions))\n",
    "else:\n",
    "    print(f\"Video folder path '{VIDEO_FOLDER_PATH}' does not exist!\")\n",
    "    print(\"Please update the VIDEO_FOLDER_PATH variable with the correct path to your video folder.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found exercise classes: {actions}\")\n",
    "num_classes = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f377e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos and extracting keypoints...\n",
      "Processing curl videos...\n",
      "Found 102 videos for curl\n",
      "Processing 1 .mp4 (1/102)\n",
      "Processing 10.mp4 (2/102)\n",
      "Processing 11.mp4 (3/102)\n",
      "Processing 12.mp4 (4/102)\n",
      "Processing 13.mp4 (5/102)\n",
      "Processing 14.mp4 (6/102)\n",
      "Processing 15.mp4 (7/102)\n",
      "Processing 2 .mp4 (8/102)\n",
      "Processing 3.mp4 (9/102)\n",
      "Processing 4 .mp4 (10/102)\n",
      "Processing 5 .mp4 (11/102)\n",
      "Processing 6.mp4 (12/102)\n",
      "Processing 7.mp4 (13/102)\n",
      "Processing 8.mp4 (14/102)\n",
      "Processing 9.mp4 (15/102)\n",
      "Processing barbell biceps curl_1.mp4 (16/102)\n",
      "Processing barbell biceps curl_10.mp4 (17/102)\n",
      "Processing barbell biceps curl_11.mp4 (18/102)\n",
      "Processing barbell biceps curl_12.mp4 (19/102)\n",
      "Processing barbell biceps curl_13.mp4 (20/102)\n",
      "Processing barbell biceps curl_14.mp4 (21/102)\n",
      "Processing barbell biceps curl_15.mp4 (22/102)\n",
      "Processing barbell biceps curl_16.mp4 (23/102)\n",
      "Processing barbell biceps curl_17.mp4 (24/102)\n",
      "Processing barbell biceps curl_18.mp4 (25/102)\n",
      "Processing barbell biceps curl_19.mp4 (26/102)\n",
      "Processing barbell biceps curl_2.mp4 (27/102)\n",
      "Processing barbell biceps curl_20.mp4 (28/102)\n",
      "Processing barbell biceps curl_21.mp4 (29/102)\n",
      "Processing barbell biceps curl_22.mp4 (30/102)\n",
      "Processing barbell biceps curl_23.mp4 (31/102)\n",
      "Processing barbell biceps curl_24.mp4 (32/102)\n",
      "Processing barbell biceps curl_25.mp4 (33/102)\n",
      "Processing barbell biceps curl_26.mp4 (34/102)\n",
      "Processing barbell biceps curl_27.mp4 (35/102)\n",
      "Processing barbell biceps curl_28.mp4 (36/102)\n",
      "Processing barbell biceps curl_29.mp4 (37/102)\n",
      "Processing barbell biceps curl_3.mp4 (38/102)\n",
      "Processing barbell biceps curl_30.mp4 (39/102)\n",
      "Processing barbell biceps curl_31.mp4 (40/102)\n",
      "Processing barbell biceps curl_32.mp4 (41/102)\n",
      "Processing barbell biceps curl_33.mp4 (42/102)\n",
      "Processing barbell biceps curl_34.mp4 (43/102)\n",
      "Processing barbell biceps curl_35.mp4 (44/102)\n",
      "Processing barbell biceps curl_36.mp4 (45/102)\n",
      "Processing barbell biceps curl_37.mp4 (46/102)\n",
      "Processing barbell biceps curl_38.mp4 (47/102)\n",
      "Processing barbell biceps curl_39.mp4 (48/102)\n",
      "Processing barbell biceps curl_4.mp4 (49/102)\n",
      "Processing barbell biceps curl_40.mp4 (50/102)\n",
      "Processing barbell biceps curl_41.mp4 (51/102)\n",
      "Processing barbell biceps curl_42.mp4 (52/102)\n",
      "Processing barbell biceps curl_43.mp4 (53/102)\n",
      "Processing barbell biceps curl_44.mp4 (54/102)\n",
      "Processing barbell biceps curl_45.mp4 (55/102)\n",
      "Processing barbell biceps curl_46.mp4 (56/102)\n",
      "Processing barbell biceps curl_47.mp4 (57/102)\n",
      "Processing barbell biceps curl_48.mp4 (58/102)\n",
      "Processing barbell biceps curl_49.mp4 (59/102)\n",
      "Processing barbell biceps curl_5.mp4 (60/102)\n",
      "Processing barbell biceps curl_50.mp4 (61/102)\n",
      "Processing barbell biceps curl_51.mp4 (62/102)\n",
      "Processing barbell biceps curl_52.mp4 (63/102)\n",
      "Processing barbell biceps curl_53.mp4 (64/102)\n",
      "Processing barbell biceps curl_54.mp4 (65/102)\n",
      "Processing barbell biceps curl_55.mp4 (66/102)\n",
      "Processing barbell biceps curl_56.mp4 (67/102)\n",
      "Processing barbell biceps curl_57.mp4 (68/102)\n",
      "Processing barbell biceps curl_58.mp4 (69/102)\n",
      "Processing barbell biceps curl_59.mp4 (70/102)\n",
      "Processing barbell biceps curl_6.mp4 (71/102)\n",
      "Processing barbell biceps curl_60.mp4 (72/102)\n",
      "Processing barbell biceps curl_61.mp4 (73/102)\n",
      "Processing barbell biceps curl_62.mp4 (74/102)\n",
      "Processing barbell biceps curl_7.mp4 (75/102)\n",
      "Processing barbell biceps curl_8.mp4 (76/102)\n",
      "Processing barbell biceps curl_9.mp4 (77/102)\n",
      "Processing bicep curl_yt1.mp4 (78/102)\n",
      "Processing bicep_closeup_front.mp4 (79/102)\n",
      "Processing bicep_closeup_frontleft.mp4 (80/102)\n",
      "Processing bicep_closeup_frontright.mp4 (81/102)\n",
      "Processing bicep_closeup_left.mp4 (82/102)\n",
      "Processing bicep_h1.mp4 (83/102)\n",
      "Processing bicep_high_front.mp4 (84/102)\n",
      "Processing bicep_high_frontleft.mp4 (85/102)\n",
      "Processing bicep_high_frontright.mp4 (86/102)\n",
      "Processing bicep_high_left.mp4 (87/102)\n",
      "Processing bicep_high_right.mp4 (88/102)\n",
      "Processing bicep_l1.mp4 (89/102)\n",
      "Processing bicep_m1.mp4 (90/102)\n",
      "Processing bicep_top_front.mp4 (91/102)\n",
      "Processing bicep_top_frontleft.mp4 (92/102)\n",
      "Processing bicep_top_frontright.mp4 (93/102)\n",
      "Processing bicep_top_left.mp4 (94/102)\n",
      "Processing bicep_top_right.mp4 (95/102)\n",
      "Processing curl_middle_front.mp4 (96/102)\n",
      "Processing curl_middle_frontleft.mp4 (97/102)\n",
      "Processing curl_middle_frontright.mp4 (98/102)\n",
      "Processing curl_middle_left.mp4 (99/102)\n",
      "Processing curl_middle_right.mp4 (100/102)\n",
      "Processing remake_correct-1_1.mp4 (101/102)\n",
      "Processing remake_correct-2_1.mp4 (102/102)\n",
      "Processing lunge videos...\n",
      "Found 66 videos for lunge\n",
      "Processing 1.mp4 (1/66)\n",
      "Processing 10.mp4 (2/66)\n",
      "Processing 11.mp4 (3/66)\n",
      "Processing 12.mp4 (4/66)\n",
      "Processing 13.mp4 (5/66)\n",
      "Processing 14.mp4 (6/66)\n",
      "Processing 15.mp4 (7/66)\n",
      "Processing 2 .mp4 (8/66)\n",
      "Processing 3 .mp4 (9/66)\n",
      "Processing 4 .mp4 (10/66)\n",
      "Processing 5.mp4 (11/66)\n",
      "Processing 6.mp4 (12/66)\n",
      "Processing 7.mp4 (13/66)\n",
      "Processing 8.mp4 (14/66)\n",
      "Processing 9.mp4 (15/66)\n",
      "Processing handlunge_high_front.mp4 (16/66)\n",
      "Processing handlunge_high_left.mp4 (17/66)\n",
      "Processing handlunge_high_right.mp4 (18/66)\n",
      "Processing handlunge_middle_front.mp4 (19/66)\n",
      "Processing handlunge_middle_frontleft.mp4 (20/66)\n",
      "Processing handlunge_middle_frontleft_ver2.mp4 (21/66)\n",
      "Processing handlunge_middle_frontright.mp4 (22/66)\n",
      "Processing handlunge_middle_frontright_ver2.mp4 (23/66)\n",
      "Processing handlunge_middle_left.mp4 (24/66)\n",
      "Processing handlunge_middle_right.mp4 (25/66)\n",
      "Processing lunge1.mp4 (26/66)\n",
      "Processing lunge10.mp4 (27/66)\n",
      "Processing lunge11.mp4 (28/66)\n",
      "Processing lunge12.mp4 (29/66)\n",
      "Processing lunge13.mp4 (30/66)\n",
      "Processing lunge14.mp4 (31/66)\n",
      "Processing lunge15.mp4 (32/66)\n",
      "Processing lunge16.mp4 (33/66)\n",
      "Processing Lunge17.mp4 (34/66)\n",
      "Processing lunge18.mp4 (35/66)\n",
      "Processing lunge19.mp4 (36/66)\n",
      "Processing lunge2.mp4 (37/66)\n",
      "Processing lunge20.mp4 (38/66)\n",
      "Processing lunge21.mp4 (39/66)\n",
      "Processing lunge3.mp4 (40/66)\n",
      "Processing lunge4.mp4 (41/66)\n",
      "Processing lunge5.mp4 (42/66)\n",
      "Processing lunge6.mp4 (43/66)\n",
      "Processing lunge7.mp4 (44/66)\n",
      "Processing lunge8.mp4 (45/66)\n",
      "Processing lunge9.mp4 (46/66)\n",
      "Processing lunge_h1.mp4 (47/66)\n",
      "Processing lunge_high_front.mp4 (48/66)\n",
      "Processing lunge_high_left.mp4 (49/66)\n",
      "Processing lunge_high_right.mp4 (50/66)\n",
      "Processing lunge_l.mp4 (51/66)\n",
      "Processing lunge_l1.mp4 (52/66)\n",
      "Processing lunge_m.mp4 (53/66)\n",
      "Processing lunge_m1.mp4 (54/66)\n",
      "Processing lunge_middle_front.mp4 (55/66)\n",
      "Processing lunge_middle_frontleft.mp4 (56/66)\n",
      "Processing lunge_middle_frontleft_ver2.mp4 (57/66)\n",
      "Processing lunge_middle_frontright.mp4 (58/66)\n",
      "Processing lunge_middle_frontright_ver2.mp4 (59/66)\n",
      "Processing lunge_middle_left.mp4 (60/66)\n",
      "Processing lunge_middle_right.mp4 (61/66)\n",
      "Processing phuclunge_high_front.mp4 (62/66)\n",
      "Processing phuclunge_high_frontleft.mp4 (63/66)\n",
      "Processing phuclunge_high_frontright_1.mp4 (64/66)\n",
      "Processing phuclunge_high_left.mp4 (65/66)\n",
      "Processing phuclunge_high_right.mp4 (66/66)\n",
      "Processing plank videos...\n",
      "Found 53 videos for plank\n",
      "Processing 1.mp4 (1/53)\n",
      "Processing 10.mp4 (2/53)\n",
      "Processing 11.mp4 (3/53)\n",
      "Processing 12.mp4 (4/53)\n",
      "Processing 13.mp4 (5/53)\n",
      "Processing 14.mp4 (6/53)\n",
      "Processing 15.mp4 (7/53)\n",
      "Processing 2.mp4 (8/53)\n",
      "Processing 3.mp4 (9/53)\n",
      "Processing 4.mp4 (10/53)\n",
      "Processing 5.mp4 (11/53)\n",
      "Processing 6.mp4 (12/53)\n",
      "Processing 7.mp4 (13/53)\n",
      "Processing 8.mp4 (14/53)\n",
      "Processing 9.mp4 (15/53)\n",
      "Processing phucplank_high_front.mp4 (16/53)\n",
      "Processing phucplank_high_frontleft.mp4 (17/53)\n",
      "Processing phucplank_high_frontright.mp4 (18/53)\n",
      "Processing phucplank_high_left.mp4 (19/53)\n",
      "Processing phucplank_high_right.mp4 (20/53)\n",
      "Processing plank 10.mp4 (21/53)\n",
      "Processing plank 11.mp4 (22/53)\n",
      "Processing plank 12.mp4 (23/53)\n",
      "Processing plank 13.mp4 (24/53)\n",
      "Processing plank 14.mp4 (25/53)\n",
      "Processing plank 5.mp4 (26/53)\n",
      "Processing plank 6.mp4 (27/53)\n",
      "Processing plank 7.mp4 (28/53)\n",
      "Processing plank 8.mp4 (29/53)\n",
      "Processing plank 9.mp4 (30/53)\n",
      "Processing plank yt_1.mp4 (31/53)\n",
      "Processing plank yt_2.mp4 (32/53)\n",
      "Processing plank yt_3.mp4 (33/53)\n",
      "Processing plank yt_4.mp4 (34/53)\n",
      "Processing plank_1.MOV (35/53)\n",
      "Processing plank_2.mp4 (36/53)\n",
      "Processing plank_3.mp4 (37/53)\n",
      "Processing plank_4.mp4 (38/53)\n",
      "Processing plank_5.mp4 (39/53)\n",
      "Processing plank_6.mp4 (40/53)\n",
      "Processing plank_7.mp4 (41/53)\n",
      "Processing plank_l1.mp4 (42/53)\n",
      "Processing plank_l2.mp4 (43/53)\n",
      "Processing plank_middle_front.mp4 (44/53)\n",
      "Processing plank_middle_frontleft.mp4 (45/53)\n",
      "Processing plank_middle_frontright.mp4 (46/53)\n",
      "Processing plank_middle_left.mp4 (47/53)\n",
      "Processing plank_middle_right.mp4 (48/53)\n",
      "Processing plank_top_front.mp4 (49/53)\n",
      "Processing plank_top_frontleft.mp4 (50/53)\n",
      "Processing plank_top_frontright.mp4 (51/53)\n",
      "Processing plank_top_left.mp4 (52/53)\n",
      "Processing plank_top_right.mp4 (53/53)\n",
      "Processing situp videos...\n",
      "Found 52 videos for situp\n",
      "Processing 1.mp4 (1/52)\n",
      "Processing 10.mp4 (2/52)\n",
      "Processing 11.mp4 (3/52)\n",
      "Processing 12.mp4 (4/52)\n",
      "Processing 13.mp4 (5/52)\n",
      "Processing 14.mp4 (6/52)\n",
      "Processing 15.mp4 (7/52)\n",
      "Processing 2.mp4 (8/52)\n",
      "Processing 3.mp4 (9/52)\n",
      "Processing 4.mp4 (10/52)\n",
      "Processing 5.mp4 (11/52)\n",
      "Processing 6.mp4 (12/52)\n",
      "Processing 7.mp4 (13/52)\n",
      "Processing 8.mp4 (14/52)\n",
      "Processing 9.mp4 (15/52)\n",
      "Processing phucsitup_high_innerleft.mp4 (16/52)\n",
      "Processing phucsitup_high_left.mp4 (17/52)\n",
      "Processing situp1.mp4 (18/52)\n",
      "Processing situp10.mp4 (19/52)\n",
      "Processing situp11.mp4 (20/52)\n",
      "Processing situp12.mp4 (21/52)\n",
      "Processing situp2.mp4 (22/52)\n",
      "Processing situp3.mp4 (23/52)\n",
      "Processing situp4.mp4 (24/52)\n",
      "Processing situp5.mp4 (25/52)\n",
      "Processing situp6.mp4 (26/52)\n",
      "Processing situp7.mp4 (27/52)\n",
      "Processing situp8.mp4 (28/52)\n",
      "Processing situp9.mp4 (29/52)\n",
      "Processing situp_a.mp4 (30/52)\n",
      "Processing situp_a1.mp4 (31/52)\n",
      "Processing situp_a2.mp4 (32/52)\n",
      "Processing situp_a3.mp4 (33/52)\n",
      "Processing situp_a4.mp4 (34/52)\n",
      "Processing situp_b.mp4 (35/52)\n",
      "Processing situp_c.mp4 (36/52)\n",
      "Processing situp_d.mp4 (37/52)\n",
      "Processing situp_e.mp4 (38/52)\n",
      "Processing situp_f.mp4 (39/52)\n",
      "Processing situp_g.mp4 (40/52)\n",
      "Processing situp_h.mp4 (41/52)\n",
      "Processing situp_l.mp4 (42/52)\n",
      "Processing situp_l1.mp4 (43/52)\n",
      "Processing situp_l2.mp4 (44/52)\n",
      "Processing situp_l3.mp4 (45/52)\n",
      "Processing situp_middleclose_headleft.mp4 (46/52)\n",
      "Processing situp_middle_headinner_front.mp4 (47/52)\n",
      "Processing situp_middle_headinner_left.mp4 (48/52)\n",
      "Processing situp_middle_headleft.mp4 (49/52)\n",
      "Processing situp_middle_headouter_front.mp4 (50/52)\n",
      "Processing situp_middle_headouter_right.mp4 (51/52)\n",
      "Processing situp_middle_headright.mp4 (52/52)\n",
      "Processing squat videos...\n",
      "Found 65 videos for squat\n",
      "Processing 1.mp4 (1/65)\n",
      "Processing 10.mp4 (2/65)\n",
      "Processing 11.mp4 (3/65)\n",
      "Processing 12.mp4 (4/65)\n",
      "Processing 13.mp4 (5/65)\n",
      "Processing 14.mp4 (6/65)\n",
      "Processing 15.mp4 (7/65)\n",
      "Processing 2 .mp4 (8/65)\n",
      "Processing 3 .mp4 (9/65)\n",
      "Processing 4 .mp4 (10/65)\n",
      "Processing 5 .mp4 (11/65)\n",
      "Processing 6.mp4 (12/65)\n",
      "Processing 7.mp4 (13/65)\n",
      "Processing 8.mp4 (14/65)\n",
      "Processing 9.mp4 (15/65)\n",
      "Processing handsquat_middle_front.mp4 (16/65)\n",
      "Processing handsquat_middle_frontleft.mp4 (17/65)\n",
      "Processing handsquat_middle_frontright.mp4 (18/65)\n",
      "Processing handsquat_middle_left.mp4 (19/65)\n",
      "Processing handsquat_middle_right.mp4 (20/65)\n",
      "Processing handsquat_top_front.mp4 (21/65)\n",
      "Processing handsquat_top_frontleft.mp4 (22/65)\n",
      "Processing handsquat_top_frontright.mp4 (23/65)\n",
      "Processing handsquat_top_left.mp4 (24/65)\n",
      "Processing handsquat_top_right.mp4 (25/65)\n",
      "Processing phucsquat_high_front.mp4 (26/65)\n",
      "Processing phucsquat_high_frontleft.mp4 (27/65)\n",
      "Processing phucsquat_high_frontright.mp4 (28/65)\n",
      "Processing phucsquat_high_left.mp4 (29/65)\n",
      "Processing phucsquat_high_right.mp4 (30/65)\n",
      "Processing squat1.mp4 (31/65)\n",
      "Processing squat2.mp4 (32/65)\n",
      "Processing squat3.mp4 (33/65)\n",
      "Processing squat_10.mp4 (34/65)\n",
      "Processing squat_11.mp4 (35/65)\n",
      "Processing squat_12.mp4 (36/65)\n",
      "Processing squat_13.mp4 (37/65)\n",
      "Processing squat_14.mp4 (38/65)\n",
      "Processing squat_15.mp4 (39/65)\n",
      "Processing squat_16.mp4 (40/65)\n",
      "Processing squat_17.mp4 (41/65)\n",
      "Processing squat_18.mp4 (42/65)\n",
      "Processing squat_19.mp4 (43/65)\n",
      "Processing squat_2.mov (44/65)\n",
      "Processing squat_20.mp4 (45/65)\n",
      "Processing squat_21.mp4 (46/65)\n",
      "Processing squat_23.mp4 (47/65)\n",
      "Processing squat_25.mp4 (48/65)\n",
      "Processing squat_8.mp4 (49/65)\n",
      "Processing squat_9.mp4 (50/65)\n",
      "Processing squat_h1.mp4 (51/65)\n",
      "Processing squat_l.mp4 (52/65)\n",
      "Processing squat_l1.mp4 (53/65)\n",
      "Processing squat_l2.mp4 (54/65)\n",
      "Processing squat_m1.mp4 (55/65)\n",
      "Processing squat_middle_front.mp4 (56/65)\n",
      "Processing squat_middle_frontleft.mp4 (57/65)\n",
      "Processing squat_middle_frontright.mp4 (58/65)\n",
      "Processing squat_middle_left.mp4 (59/65)\n",
      "Processing squat_middle_right.mp4 (60/65)\n",
      "Processing squat_top_front.mp4 (61/65)\n",
      "Processing squat_top_frontleft.mp4 (62/65)\n",
      "Processing squat_top_frontright.mp4 (63/65)\n",
      "Processing squat_top_left.mp4 (64/65)\n",
      "Processing squat_top_right.mp4 (65/65)\n",
      "Finished processing all videos!\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 30\n",
    "\n",
    "# Colors associated with each exercise\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "while len(colors) < len(actions):\n",
    "    colors.append((np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255)))\n",
    "\n",
    "# Process videos and extract keypoints\n",
    "print(\"Processing videos and extracting keypoints...\")\n",
    "\n",
    "# Build folder paths for processed data\n",
    "for action in actions:     \n",
    "    try: \n",
    "        os.makedirs(os.path.join(DATA_PATH, action))  \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Process each video and extract keypoints\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    sequence_counter = 0\n",
    "    \n",
    "    for idx, action in enumerate(actions):\n",
    "        action_folder = os.path.join(VIDEO_FOLDER_PATH, action)\n",
    "        print(f\"Processing {action} videos...\")\n",
    "        \n",
    "        # Get all video files in the action folder\n",
    "        video_files = []\n",
    "        for file in os.listdir(action_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in VIDEO_EXTENSIONS):\n",
    "                video_files.append(file)\n",
    "        \n",
    "        print(f\"Found {len(video_files)} videos for {action}\")\n",
    "        \n",
    "        for video_idx, video_file in enumerate(video_files):\n",
    "            video_path = os.path.join(action_folder, video_file)\n",
    "            print(f\"Processing {video_file} ({video_idx + 1}/{len(video_files)})\")\n",
    "            \n",
    "            try:\n",
    "                # Process video and extract keypoints\n",
    "                keypoints_sequence = process_video_to_keypoints(video_path, sequence_length, pose)\n",
    "                \n",
    "                # Create sequence folder\n",
    "                sequence_folder = os.path.join(DATA_PATH, action, str(sequence_counter))\n",
    "                os.makedirs(sequence_folder, exist_ok=True)\n",
    "                \n",
    "                # Save each frame's keypoints\n",
    "                for frame_num, keypoints in enumerate(keypoints_sequence):\n",
    "                    npy_path = os.path.join(sequence_folder, f\"{frame_num}.npy\")\n",
    "                    np.save(npy_path, keypoints)\n",
    "                \n",
    "                sequence_counter += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "print(\"Finished processing all videos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa63162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label map\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3460d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed keypoint data...\n"
     ]
    }
   ],
   "source": [
    "# Load and organize processed training data\n",
    "print(\"Loading processed keypoint data...\")\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    action_data_path = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(action_data_path):\n",
    "        continue\n",
    "        \n",
    "    sequence_folders = [f for f in os.listdir(action_data_path) \n",
    "                       if os.path.isdir(os.path.join(action_data_path, f))]\n",
    "    \n",
    "    for sequence_folder in sequence_folders:\n",
    "        sequence_path = os.path.join(action_data_path, sequence_folder)\n",
    "        window = []\n",
    "        \n",
    "        # Load keypoints for each frame in the sequence\n",
    "        for frame_num in range(sequence_length):\n",
    "            frame_path = os.path.join(sequence_path, f\"{frame_num}.npy\")\n",
    "            if os.path.exists(frame_path):\n",
    "                res = np.load(frame_path)\n",
    "                window.append(res)\n",
    "            else:\n",
    "                # If frame doesn't exist, use zeros\n",
    "                window.append(np.zeros(33*4))\n",
    "        \n",
    "        if len(window) == sequence_length:\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa56d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: X=(338, 30, 132), y=(338, 5)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "    print(\"No data found! Please check your video folder structure and paths.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9017312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: X_train=(304, 30, 132), y_train=(304, 5)\n",
      "Validation set: X_val=(51, 30, 132), y_val=(51, 5)\n",
      "Test set: X_test=(34, 30, 132), y_test=(34, 5)\n"
     ]
    }
   ],
   "source": [
    "# Calculate input dimensions\n",
    "num_input_values = X.shape[2]  # Should be 33*4 = 132 for pose landmarks\n",
    "\n",
    "# Split into training, validation, and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)\n",
    "print(f\"Validation set: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa487a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to be used during neural network training \n",
    "es_callback = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, verbose=0, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0, mode='min')\n",
    "# Fix the checkpoint callback to use proper file path\n",
    "checkpoint_path = os.path.join(os.getcwd(), 'best_model.keras')\n",
    "chkpt_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3143aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "max_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "259f5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]\n",
    "\n",
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cff99cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(sequence_length, num_input_values))\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "\n",
    "# Attention\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output\n",
    "x = Dense(actions.shape[0], activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e4a2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83632c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">796,672</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │ permute[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_vec       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mul       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ attention_vec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15360</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_mul[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,864,832</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m132\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m796,672\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ permute (\u001b[38;5;33mPermute\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │        \u001b[38;5;34m930\u001b[0m │ permute[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_vec       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPermute\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mul       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ attention_vec[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15360\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ attention_mul[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m7,864,832\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │      \u001b[38;5;34m2,565\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,664,999</span> (33.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,664,999\u001b[0m (33.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,664,999</span> (33.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,664,999\u001b[0m (33.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Starting model training...\n",
      "Epoch 1/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - categorical_accuracy: 0.2006 - loss: 2.4295 - val_categorical_accuracy: 0.3137 - val_loss: 1.4574 - learning_rate: 0.0100\n",
      "Epoch 2/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - categorical_accuracy: 0.3642 - loss: 1.4908 - val_categorical_accuracy: 0.4706 - val_loss: 1.1303 - learning_rate: 0.0100\n",
      "Epoch 3/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - categorical_accuracy: 0.4812 - loss: 1.0907 - val_categorical_accuracy: 0.6863 - val_loss: 0.8802 - learning_rate: 0.0100\n",
      "Epoch 4/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.6194 - loss: 0.9520 - val_categorical_accuracy: 0.6078 - val_loss: 1.0198 - learning_rate: 0.0100\n",
      "Epoch 5/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - categorical_accuracy: 0.5856 - loss: 0.9551 - val_categorical_accuracy: 0.6667 - val_loss: 0.7414 - learning_rate: 0.0100\n",
      "Epoch 6/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - categorical_accuracy: 0.6390 - loss: 0.8066 - val_categorical_accuracy: 0.6471 - val_loss: 0.7284 - learning_rate: 0.0100\n",
      "Epoch 7/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - categorical_accuracy: 0.6537 - loss: 0.7376 - val_categorical_accuracy: 0.5882 - val_loss: 0.8581 - learning_rate: 0.0100\n",
      "Epoch 8/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - categorical_accuracy: 0.6434 - loss: 0.7673 - val_categorical_accuracy: 0.6863 - val_loss: 0.7565 - learning_rate: 0.0100\n",
      "Epoch 9/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - categorical_accuracy: 0.7255 - loss: 0.6055 - val_categorical_accuracy: 0.8039 - val_loss: 0.5977 - learning_rate: 0.0100\n",
      "Epoch 10/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - categorical_accuracy: 0.7681 - loss: 0.5524 - val_categorical_accuracy: 0.7059 - val_loss: 0.6277 - learning_rate: 0.0100\n",
      "Epoch 11/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - categorical_accuracy: 0.7678 - loss: 0.4672 - val_categorical_accuracy: 0.7647 - val_loss: 0.5642 - learning_rate: 0.0100\n",
      "Epoch 12/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.8211 - loss: 0.3948 - val_categorical_accuracy: 0.7451 - val_loss: 0.6897 - learning_rate: 0.0100\n",
      "Epoch 13/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - categorical_accuracy: 0.8280 - loss: 0.3898 - val_categorical_accuracy: 0.7451 - val_loss: 0.7372 - learning_rate: 0.0100\n",
      "Epoch 14/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.8270 - loss: 0.4583 - val_categorical_accuracy: 0.7843 - val_loss: 0.7005 - learning_rate: 0.0100\n",
      "Epoch 15/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - categorical_accuracy: 0.8255 - loss: 0.4291 - val_categorical_accuracy: 0.6863 - val_loss: 0.6797 - learning_rate: 0.0100\n",
      "Epoch 16/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - categorical_accuracy: 0.8529 - loss: 0.3775 - val_categorical_accuracy: 0.8235 - val_loss: 0.5045 - learning_rate: 0.0100\n",
      "Epoch 17/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - categorical_accuracy: 0.8829 - loss: 0.2872 - val_categorical_accuracy: 0.6863 - val_loss: 0.7087 - learning_rate: 0.0100\n",
      "Epoch 18/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - categorical_accuracy: 0.9227 - loss: 0.2035 - val_categorical_accuracy: 0.7059 - val_loss: 0.8599 - learning_rate: 0.0100\n",
      "Epoch 19/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - categorical_accuracy: 0.8867 - loss: 0.2668 - val_categorical_accuracy: 0.7647 - val_loss: 0.7072 - learning_rate: 0.0100\n",
      "Epoch 20/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - categorical_accuracy: 0.8895 - loss: 0.2156 - val_categorical_accuracy: 0.7647 - val_loss: 0.8333 - learning_rate: 0.0100\n",
      "Epoch 21/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - categorical_accuracy: 0.8914 - loss: 0.2558 - val_categorical_accuracy: 0.8431 - val_loss: 0.6089 - learning_rate: 0.0100\n",
      "Epoch 22/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - categorical_accuracy: 0.9297 - loss: 0.1610 - val_categorical_accuracy: 0.8235 - val_loss: 0.5780 - learning_rate: 0.0020\n",
      "Epoch 23/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - categorical_accuracy: 0.9768 - loss: 0.0885 - val_categorical_accuracy: 0.8431 - val_loss: 0.5161 - learning_rate: 0.0020\n",
      "Epoch 24/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - categorical_accuracy: 0.9795 - loss: 0.0933 - val_categorical_accuracy: 0.8039 - val_loss: 0.4719 - learning_rate: 0.0020\n",
      "Epoch 25/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - categorical_accuracy: 0.9811 - loss: 0.0613 - val_categorical_accuracy: 0.8235 - val_loss: 0.4869 - learning_rate: 0.0020\n",
      "Epoch 26/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - categorical_accuracy: 0.9750 - loss: 0.0751 - val_categorical_accuracy: 0.8431 - val_loss: 0.5054 - learning_rate: 0.0020\n",
      "Epoch 27/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.9913 - loss: 0.0515 - val_categorical_accuracy: 0.8431 - val_loss: 0.4999 - learning_rate: 0.0020\n",
      "Epoch 28/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - categorical_accuracy: 0.9873 - loss: 0.0592 - val_categorical_accuracy: 0.8431 - val_loss: 0.4846 - learning_rate: 0.0020\n",
      "Epoch 29/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - categorical_accuracy: 0.9986 - loss: 0.0464 - val_categorical_accuracy: 0.8431 - val_loss: 0.5004 - learning_rate: 0.0020\n",
      "Epoch 30/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - categorical_accuracy: 0.9918 - loss: 0.0310 - val_categorical_accuracy: 0.8431 - val_loss: 0.5077 - learning_rate: 4.0000e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - categorical_accuracy: 0.9863 - loss: 0.0374 - val_categorical_accuracy: 0.8627 - val_loss: 0.5036 - learning_rate: 4.0000e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - categorical_accuracy: 0.9991 - loss: 0.0317 - val_categorical_accuracy: 0.8431 - val_loss: 0.5002 - learning_rate: 4.0000e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - categorical_accuracy: 0.9913 - loss: 0.0395 - val_categorical_accuracy: 0.8431 - val_loss: 0.5009 - learning_rate: 4.0000e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - categorical_accuracy: 0.9974 - loss: 0.0349 - val_categorical_accuracy: 0.8431 - val_loss: 0.5009 - learning_rate: 4.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())\n",
    "\n",
    "AttnLSTM.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = AttnLSTM.fit(X_train, y_train, \n",
    "                       batch_size=batch_size, \n",
    "                       epochs=max_epochs, \n",
    "                       validation_data=(X_val, y_val), \n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eecc393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model map\n",
    "models = {\n",
    "    'LSTM_Attention_128HUs': AttnLSTM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37ead88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: c:\\Users\\pheno\\Downloads\\work\\main_model\\LSTM_Attention_128HUs.h5\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)\n",
    "    print(f\"Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6d4c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model rebuild before doing this\n",
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00ba3261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "LSTM_Attention_128HUs Test Accuracy: 0.8235\n",
      "Classes in test set: ['curl', 'lunge', 'plank', 'situp', 'squat']\n",
      "Classes predicted: ['curl', 'lunge', 'plank', 'situp', 'squat']\n",
      "Total unique classes in evaluation: 5\n",
      "\n",
      "Classification Report for LSTM_Attention_128HUs:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        curl       0.75      0.75      0.75         4\n",
      "       lunge       0.75      0.90      0.82        10\n",
      "       plank       1.00      0.75      0.86         4\n",
      "       situp       0.88      0.88      0.88         8\n",
      "       squat       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.82        34\n",
      "   macro avg       0.85      0.81      0.82        34\n",
      "weighted avg       0.83      0.82      0.82        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model on test set...\")\n",
    "for model_name, model in models.items():\n",
    "    test_predictions = model.predict(X_test, verbose=0)\n",
    "    test_predictions = np.argmax(test_predictions, axis=1)\n",
    "    test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Check what classes are present in test set\n",
    "    unique_test_labels = np.unique(test_labels)\n",
    "    unique_pred_labels = np.unique(test_predictions)\n",
    "    all_unique_labels = np.unique(np.concatenate([unique_test_labels, unique_pred_labels]))\n",
    "    \n",
    "    print(f\"Classes in test set: {[actions[i] for i in unique_test_labels]}\")\n",
    "    print(f\"Classes predicted: {[actions[i] for i in unique_pred_labels]}\")\n",
    "    print(f\"Total unique classes in evaluation: {len(all_unique_labels)}\")\n",
    "    \n",
    "    # Print classification report with proper labels\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    try:\n",
    "        # Use only the labels that are actually present\n",
    "        present_target_names = [actions[i] for i in all_unique_labels]\n",
    "        print(classification_report(test_labels, test_predictions, \n",
    "                                  labels=all_unique_labels,\n",
    "                                  target_names=present_target_names))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate full classification report: {e}\")\n",
    "        print(\"Generating simplified report...\")\n",
    "        \n",
    "        # Fallback: just show accuracy and basic metrics\n",
    "        from collections import Counter\n",
    "        test_counts = Counter(test_labels)\n",
    "        pred_counts = Counter(test_predictions)\n",
    "        \n",
    "        print(f\"Test label distribution: {[(actions[k], v) for k, v in test_counts.items()]}\")\n",
    "        print(f\"Prediction distribution: {[(actions[k], v) for k, v in pred_counts.items()]}\")\n",
    "        \n",
    "        # Show confusion matrix\n",
    "        try:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(test_labels, test_predictions, labels=all_unique_labels)\n",
    "            print(f\"Confusion Matrix:\")\n",
    "            print(f\"Labels: {[actions[i] for i in all_unique_labels]}\")\n",
    "            print(cm)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ea5ec",
   "metadata": {},
   "source": [
    "/Eval tab to check \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9235ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7890074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Attention_128HUs confusion matrix: \n",
      "[[[29  1]\n",
      "  [ 1  3]]\n",
      "\n",
      " [[21  3]\n",
      "  [ 1  9]]\n",
      "\n",
      " [[30  0]\n",
      "  [ 1  3]]\n",
      "\n",
      " [[25  1]\n",
      "  [ 1  7]]\n",
      "\n",
      " [[25  1]\n",
      "  [ 2  6]]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Get unique labels present in test data\n",
    "    unique_labels = np.unique(np.concatenate([ytrue, yhat]))\n",
    "    \n",
    "    # Confusion matrix - handle case where not all classes are present\n",
    "    try:\n",
    "        confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "        print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate confusion matrix for {model_name}: {e}\")\n",
    "        confusion_matrices[model_name] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "714124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pheno\\downloads\\work\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading pandas-2.3.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.6/11.3 MB 13.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.9/11.3 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   -------------------- ------------------- 2/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ------------------------------ --------- 3/4 [seaborn]\n",
      "   ---------------------------------------- 4/4 [seaborn]\n",
      "\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAIjCAYAAABh1T2DAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWVtJREFUeJzt3QeYE1XXwPEzS1nq0rv03sVCU0RBsCJFxIKCgLwoiAVQBKWKLoIgIkpVmigqAr42UBFBFBBpKlJEQaRJB2lL2XzPuZ/ZN9lGdtkwk5n/z2dkM0kmN3NnkpNzy1g+n88nAAAAcIUouwsAAACAjENwBwAA4CIEdwAAAC5CcAcAAOAiBHcAAAAuQnAHAADgIgR3AAAALkJwBwAA4CIEdwAAAC5CcIcM89tvv0nz5s0lT548YlmWzJ8/P0O3v337drPdadOmZeh2I9n1119vloxy/Phxeeihh6Ro0aJmXz/xxBMZtm1Enow+vgBcGgR3LvP7779Lt27dpFy5cpItWzaJiYmRa665Rl599VU5depUWF+7Y8eO8vPPP8sLL7wgM2fOlKuuukrc4sEHHzTBju7P5PajBrZ6vy4vv/xymre/e/duGTx4sKxbt07s9OKLL5rg+ZFHHjF1+MADD4T19cqUKSO33377BR/38ccfS+PGjaVw4cKSI0cOc3y3a9dOFixYYO7XAMS//1NbdB/7X1dv33jjjcm+3uTJkxOe8+OPP6brvW3cuNE8X8/DI0eOJLn/5MmTpjzffPNNkvs+++yzhLKG26+//mpeS388OdX48ePlrrvuklKlSpl9qudjchYtWiSdO3eWSpUqJRwn+mNlz549SR4bHx8vEyZMkMsvv1xy5colRYoUkVtuuUW+//77oMfp+ZDacaDHXo0aNYLW6eMfffTRZB8/Z84cc39y9Q5klMwZtiXY7tNPPzUfgNHR0dKhQwfzgXPmzBlZtmyZPPXUU7JhwwaZNGlSWF5bA57ly5fLs88+m+KH2sUqXbq0eZ0sWbKIHTJnzmy+kDXQ0MAi0KxZs8yX+OnTp9O1bQ3uhgwZYoIO/bIJ1RdffCEZ6euvv5b69evLoEGDxCk0WNbjV4O7fv36mS/trVu3yldffSWzZ8+Wm2++2Rx3+iXut2rVKhk7dqz0799fqlatmrC+Vq1aCX9rfS1evFj27t1rMpUZWZ/q7bffNts9fPiw+UIPLJ/SY0nrXCXOjmlw9/rrr1+SAE+DOy2HlkGPv3AeX+n10ksvyT///CN169ZNNlDz69u3rxw6dMh8DlasWFH++OMPGTdunHzyySfmh1NgPesxNXr0aLn//vule/fuJgCfOHGiOc6+++4781pApCK4c4lt27bJPffcYwIg/YIuVqxYwn09evQwX4Ya/IXL/v37zb958+YN22v4syB20aBZs6DvvvtukuDunXfekdtuu00+/PDDS1IWDQw0yMmaNWuGbnffvn1SrVq1DNveuXPnTIYkveXU5z///PPSrFmzZAMNLa/S+wPpcaLBna5PqVlR61KDwPfee08ef/zxhPU7d+6Ub7/9Vlq3bp3u+vT5fOaYuO+++8y5qcFi4uAuEmT08ZVeS5YsScjaaZYtJRqsXXvttRIV9b9GKQ3+NWDTIG/YsGEJx5VmA9u2bWsy1H4aFGq2T+uL4A6RjGZZlxgxYoTpL/Xmm28GBXZ+FSpUCPoC839pli9f3gQt+otdsxxxcXHJNptp9k8/7PRLUz/8ZsyYkfAYzS5oUOn/NawfwP4MgDafJM4G+J+jjwv05Zdfmg9mDRD1A7xy5cqmTBfqc6fBbKNGjSRnzpzmuS1btjRNYsm9nga5WiZ9nPYN7NSpkwmUQqVf1p9//nlQM5sGCNosq/clplmEPn36SM2aNc170mZdbfpZv359wmO0eebqq682f2t5/M2B/vfpb/ZZvXq1XHfddSao8++XxH2itGlc6yjx+7/pppskX758JkOYHC2DvqYGIvojwF8Gf1OdBlFdunQxTVe6/dq1a8v06dODtuGvH820jRkzJuHY0sxQeh04cECOHTtmArHkaDNteun7aNOmjQnCAmnwrvtK91l6aeZH94f+4NJl6dKlJmj00/sKFSpk/tasWWCzsR6fmrVTgU3Kfhos6/6tXr26eQ9aJ9oVQzOEaT139RjTgEbdcMMNCa/lbzJMrs9dWo8FbS3wHwt6nOv5klb6+ZL48yI5en4EBnb+dfnz5w86J86ePWtaAfQ9JD6e9PnZs2eXS0k/P+68806TWdR9etlll5nj5ujRo5e0HHAPMncuoU2F+sHdsGHDkB6vWQT9QNZfrr1795aVK1dKbGys+QCcN29e0GM1INLH6Qe6Bg9vvfWW+QK68sorzReMfkFqsPTkk0/KvffeK7feemuqv66To03G+kWkzWZDhw41XwT6uvolmRptmtNgSd+7fjHqB/Zrr71mgoE1a9YkCSw141a2bFnzXvX+KVOmmA90bfYJhb7Xhx9+WObOnWv69igNDqpUqSJXXHFFksdrs5AOLNEvUH3dv//+O6HpR4Oe4sWLm2ZDfc8DBw6U//znPyZQVYF1efDgQfM+9QNfm5ESfyn5ad9KDXa1nrSZPFOmTOb1NOulGQp9veRoGfR+rUP9YtFjQmkAovtUv+C1PrTJXd/HBx98YI4BDXIDfzSoqVOnmuZMfS9aj/rFml5aN/pFq8d3z549L2pbydGAXAcBaV9VDUD89anH+8U0/2vmR7enwYwG5hqQa9CoP378+1UzR9q3UTOEelwpPf5PnDhhgnD9sROYVfLTQE6DMv0h8Nhjj5mAXLNSa9euNedLYLkvdO5q4KPbSNyEHdiUHSitx4LuS21O1TJrcKY/QvW96nlxqbpX6I9eXQoWLJiwTo+pevXqmf3YoEEDc85p+fUHrwb2euwmpoGW/thITAPFi6FdZ/SHhP6w1mNcA7xdu3aZpmQtk/4IBdLMh4h39OhRn1Zly5YtQ3r8unXrzOMfeuihoPV9+vQx67/++uuEdaVLlzbrli5dmrBu3759vujoaF/v3r0T1m3bts08buTIkUHb7Nixo9lGYoMGDTKP93vllVfM7f3796dYbv9rTJ06NWHd5Zdf7itcuLDv4MGDCevWr1/vi4qK8nXo0CHJ63Xu3Dlom61bt/YVKFAgxdcMfB85c+Y0f7dt29bXtGlT8/f58+d9RYsW9Q0ZMiTZfXD69GnzmMTvQ/ff0KFDE9atWrUqyXvza9y4sblvwoQJyd6nS6CFCxeaxw8bNsz3xx9/+HLlyuVr1aqVLxRaV7fddlvQujFjxpjtvf322wnrzpw542vQoIHZ9rFjxxLelz4uJibGHCPpfb3EBg4caLar+/+WW27xvfDCC77Vq1en+pwPPvjAPGfx4sWpvu65c+dM/T3//PNm/a+//mqet2TJElMX+rfWTVrovtFj6tlnn01Yd9999/lq164d9Dg91nX7emwm1qNHj6Dzw+/bb78162fNmhW0fsGCBUnWh3rupravEh9faT0WdD8cOnQo4bEfffSRWf/xxx/70kuPAz0fQ6V1q6+5aNGioPW//fab74orrjD3+Zdy5cr5Nm3aFPQ4/3GQ2lK9evWg5+g6rcPkJN7fa9euNbd1PZBRaJZ1AW22Urlz5w7p8dpZW/Xq1StovT9bk7hvnvbB8meT/FkHbTLVX98Zxd9X76OPPjLNTqHQjtXaSVqzBoEZHc1+aF8r//sMpFm3QPq+NCvm34ehZnu02Uo74muWTP9NrklWaebK30x0/vx581r+JmfNHIZKt6OZmlBoJkozJZoN1CyJNvNo9i69dD9qNkGzsn6addGMj2ZEtD9UIG1e8jc5ZgRtttQMUJ06dWThwoVm8IRmnjRTmrj5Oa00s6nZXM2q+TNuJUuWDDre00qb7bWeA/eX/q1N8ZqhvhiaJdNMjh7fmkXyL7o/9LjSASLhPHfTeizcfffdJhPm5y9LRn52pEabw/X40Tpu0qRJ0H36eanZS+2TrJn4N954w3RXadWqVbIZOm0q12xq4iVwkE56+DNzemynpYsIkBqCOxfQflxKmz9C8eeff5qAQ/vhBdIPbQ2y9P5A2pE5Mf3ATtzH52Lol4A2pWpzsTY5avPj+++/n2qg5y+nflklps1K+gGtTVypvRf/F09a3os2O+sXg3bE12BAm94S70s/Lf8rr7xiRu5pgKZNQ/oF+9NPP6WpP02JEiXS1Lld+zppwKvBrza5XUzfNN3PWv7EfZn8TXeJjxdtqstoGkzoIAetJ21i1mBamyFbtGhxUSNalW5Lm8g1+NIgUo+9UPp3pTZKVveBv2uBLtpEq02zerxcbN8sPW60PvU4Clw0uPIPMAnXuZvWYyEjzrf02rRpk2ny1mZx7X4RSIM4nQZHAytt0tbHaRO5dvPQJvqRI0cm2Z72W9TnJF4Cg9e08B9jeqzoD20to34+aBOtBpL0t8PFoM+dS4I77Uv1yy+/pOl5oX6BaXYjOf/f+pC+19AsViDtA6O/sjXzoJlDnb9Mgyf9ta1f5imVIa0u5r346Ze2ZsS0z6JmIFKbrkLnjRswYIDpn6f9eTTg0i9GnRw41AylSmsHbw18/F/0OvdgYKYl3MLZGV2Pdc1a6aIZI60D7S+qfRjTS/teafCldaL911LKwoZCM8DaP1ADTg2CEtPgUeeBTG/wqMeMBnYpBYmJM6YZcbxfDLte/6+//kqYUF2zjYlbNfSzRj8vdXRtIK0zDVQv1Nc3lM+IlOYV9WfnAkf+jxo1yrRAaMuFft5pJlT7Ba9YscL0gQXSiuDOJXQwgo5K00702kH4QiPP9EtCswCBHae1s7924PWPfM0I+qs2uQlcE//CVxr0NG3a1Cz6oauBkTbBacCX3GSz/nJu3rw52V/t+itYR9CGgwYA2jldy6yZnpTo/GY6ClFHMQfSfRLYwftiMkWJabZSm3C1SU4HZWgnds1M+EfkppXuZ8006jETmLHRfey/3w46SbYGd6nNexYqDX51mgw9H9Iyz2Bi2ryngZ0OlgisX/9x+txzz5nAQUeFp1bnKd2nQahmlzTLnVFBdFqOPaceC4G0SVwDOx2goJMaJzd7gH7WJfcj0z9AQjN7F0P3Q3KfS8q/PvG+0hH1uugxohMpax3rJMv+6VuAtKBZ1iWefvppE8hos6b/gyuQNjXoSEp/s6LS6RQC+X/F6nxtGUW/jLR5Qb8Q/PTLOPGIXJ0yJDH/l2zi6Vn89ENbH6Nf8IEBpP4i11+//vcZDhqwaSZOm3QST4CbOHOROEuh/aZ0NFwgfxCaXCCcVjqR644dO8x+0TrVEcM6UjKl/Xghuh+1X6FmUv30y09HJWs/r4vJml2IZjn0B0tKfdtSapZPKz1vdOJmzaBcDG2S1ZHb2rdTR6kGLjolju4vf9ZNm2lTqvOUjgftO6YBiR57iWmdpOf4ScuxZ+exEOoPGy2jnl+asUsue6r0ChZKJ8EOpP1gNfjS/p0XQ8ugWTedviiQ7mOtf/3c8n9uaLY3cTCpQZ4Gz+k9ZwEydy6hQZQ2+WjfNc0+BF6hQn8F+qcrUDovlX7Za6ZPP2z0A/mHH34wwYB2JtbAJaNoVkuDDc0caVODfllrVkM/XAMHFGjnf20q0cBSf9Fqk6J2cNYmCc1ypET7xugUIZqt1Oke/FOhaHNMOGf31w9e/YUdSkZV35tm0jSLpk2k+uGuAUDi+tP+jvpLXZuQ9AtXmwvT2n9NB3joftNAxT81i05NotNXaPOwZvHSSqeF0AEZevzol5UGi5qR1AyU/kAIdSBPSrRPWnLZCf2C1X2g+02vmqGT0epgBz1mdXoZ7YOnx+vFfhErPeYu9njR6Us0y6zHeUpNddqfSs9F7QepmTfNrmqgpOeDNtnrOauLDpBQui19jv5I0HNJz1UdLKNNdtqfUjNU2jytWXjdrv6A00AyLTTQ0O3rdED6Q0zLqd0hkuunGe5jISXa1O2fG1Iza/pj0X/M3HHHHQmDGtq3b28+y7QbhA62CRxwo8GnHi9K96827etnngZXuh/1R6d+dmi9XOw1lZ955hlTHzrVjNaXTpWkx4dOvaKvo+dk4Dmr08rodEl6HGigp1PgaJ3o4CQgXTJs3C0cYcuWLb6uXbv6ypQp48uaNasvd+7cvmuuucb32muvmWk5/M6ePWum7yhbtqwvS5YsvpIlS/r69esX9JjUpqpIPEVCSlOhqC+++MJXo0YNU57KlSubaRQST4Wi0xToVC7Fixc3j9N/7733XvN+Er9G4ulCvvrqK/Mes2fPbqbhaNGihZnSIpD/9RJPteKf5kC3HepUKClJaSoUnXaiWLFipnxazuXLlyc7hYlOE1GtWjVf5syZg96nPi7xVAt+gdvRaSi0vnR6B63fQE8++aSZHkZfOzUp1ffff//t69Spk69gwYKmfmrWrJmkHlI7BlJ7vZSml+jSpYt5H5MnTzZTuehjdRqPHDly+OrUqWNeJy4u7qKmQklNWqdCGTVqVLJTbgSaNm2aeYzWtfr+++99V155pdmngdOi6BQtPXv29BUqVMhnWVaSaVEmTZpknqfHlJ7jWh9PP/20b/fu3Rd8j8kde7qPdRqQTJkyBe235B57scdCStO/XOj8S+k4CXzt1I6nxFMynTx50kxHpOec7sc8efL4br/9djM1SVqOg5TOz507d5rppkqUKGHO6fz585vtr1ixIuhxOl2RTtFUvnx5X7Zs2czjbrjhBvO5BqSXpf9LX1gIAAAAp6HPHQAAgIvQ5w4ALkD7riY36CeQ9vO81NckjWTsUyB8CO4A4AJ0UNKFBhppJ3n/oCVcGPsUCB/63AHABegVFRJPa5GYXsoquTnVkDz2KZAyveKUznCg04bp7BE6K4COhg91vlKCOwAAAAfRac10zladOkyvQKVzaOqlLPVSiXo5ygshuAMAAHAIna9V54zUy9EFXlRA52fUeV1DuWoJfe4AAADCSK82kviKIzphuC6J6UTWeiWawOsPKx1ctGzZspBez5WZuznrL/5ak8gYdUvmt7sICFA4JukHCQA4RTYbU07Z6zwatm33bVlQhgwZErROrySU0pVx9Mo8WbNmNVeeKlKkiLz77rvmylIVKlRI8brFgQjuEFYEd85CcAfAydwa3B1ZMSrkzJ3/evB6GT29LKdeik4vJ6mXp9NBSIGX1UsJzbIAAABW+K7rkFoglxy93viSJUvkxIkT5vrHOmpcB1kkvi55SrhCBQAAgGWFb0mnnDlzmsBOpw5auHChtGzZMqTnkbkDAABwEA3ktNdc5cqVZevWrfLUU09JlSpVpFOnTiE9n+AOAADAck5j5tGjR6Vfv36yc+dOyZ8/v9x5553ywgsvSJYsWUJ6PsEdAACAg7Rr184s6UVwBwAAYKW/b5zTOCcHCQAAgItG5g4AAMByT77LPe8EAAAAZO4AAADERX3uCO4AAAAs9zRmuuedAAAAgMwdAACAuKhZlswdAACAi5C5AwAAsNyT73LPOwEAAACZOwAAAKHPHQAAAJyIzB0AAIDlnnwXwR0AAIBFsywAAAAciMwdAACA5Z58l3veCQAAAMjcAQAACJk7AAAAOBGZOwAAgChGywIAAMCByNwBAABY7sl3EdwBAABYNMsCAADAgcjcAQAAWO7Jd7nnnQAAAIDMHQAAgNDnDgAAAE5E5g4AAMByT77LluCuTZs2IT927ty5YS0LAACAm9gS3OXJk8eOlwUAAHB9nztbgrupU6eaf30+n/z1119SqFAhyZ49ux1FAQAAEDc1y9r6TjS4q1ChguzcudPOYgAAALiGrcFdVFSUVKxYUQ4ePGhnMRxl5Rcfydg+nWVox1vNMuHZ7rJ57Uq7i+VJP639UQb0eVTubtFUmjWoJd8t+druInne7HdmyS3NmsjVdWpK+3vukp9/+snuInkWdeEc1EUGNstaYVouMdtzkMOHD5ennnpKfvnlF7uL4ggx+QvJTff9R7oPnyTdYydKuRpXyKwRz8rff22zu2iec/r0KSlXsbL07N3f7qJARBZ8/pm8PCJWunXvIbM/mCeVK1eRR7p14cehDagL56Au4MjgrkOHDvLDDz9I7dq1Tb+7/PnzBy1eU/WqhlL5ivpSsNhlUrB4SWl+70OSNVt2+eu3X+0umufUbdBIOnXrKdde39TuokBEZk6fKm3atpNWre+U8hUqyHODhki2bNlk/twP7S6a51AXzkFdZHCfOytMi9fmuRszZozdRXCs+Pjz8svyb+RM3GkpVam63cUBbHP2zBnZ+OsG6dK1W1C3jvr1G8pP69faWjavoS6cg7qAY4O7jh07XtTz4+LizBLo7Jk4yZI1WiLV3h1/yMRnu8u5s2dM1q59n+el8GVl7C4WYJvDRw7L+fPnpUCBAkHr9fa2bX/YVi4voi6cg7rIYBZToWSYHTt2pHp/qVKlUr0/NjZWhgwZErTurm69pN0jfSRSaXPsoyOnyOmTJ+SXFUtkzuux0nXIqwR4AADA+cFdmTJlxEolWtZfJanp16+f9OrVK2jdp5sPSSTLnDmLFCh6mfm7RLnKsuv3TfL9Zx9Kq//0trtogC3y5c0nmTJlStJJXG8XLFjQtnJ5EXXhHNSFO+e5O3/+vAwePFjefvtt2bt3rxQvXlwefPBBee6551KNlwLZ/k7Wrl0ra9asSVhWrlwpEyZMkEqVKskHH3xwwedHR0dLTExM0BLJTbLJ8cX7TBMt4FVZsmaVqtWqy8oVyxPWxcfHy8qVy6VW7Tq2ls1rqAvnoC7cOaDipZdekvHjx8u4ceNk48aN5vaIESPktddei5zMnY6STeyqq64ykerIkSPTdB1aN1j4ziSpdHk9yVuwsMSdPiXrl30l235dJw8+O9LuonnOqZMnZdfO/3Ub2Lt7l2zdskliYvJI4aLFbC2bFz3QsZMM6N9XqlevITVq1pK3Z06XU6dOSavW3vqMcALqwjmoC/f5/vvvpWXLlnLbbbcltHC+++67ZmaRiAnuUlK5cmVZtWqVeM2Jo0dkzusvyj+HD0m2HDmlaOlyJrCrUOsqu4vmOVs2bZA+Pbok3J4w9v8D7Ga33iFPDxhmY8m86eZbbpXDhw7JG+PGyoED+6VylaryxsQpUoDmp0uOunAO6iIyBlTEJTP4U1sedUmsYcOGMmnSJNmyZYtpxVy/fr0sW7ZMRo8eHfLrWT69BpiNjh07FnRbi7Nnzx7T3rxp0yZZt25dmrc5Z/2eDCwhLkbdkt6bq9DJCse4q8sCAHfJZmPKKfsd48O27b5X/J1k8OegQYNMrJOYNq3379/fNMVqn0rtg/fCCy+YMQYRk7nLmzdvkg6CGuCVLFnSpCEBAAAieUBFv2QGfyaXtVPvv/++zJo1S9555x2pXr26SXI98cQTprtaqNPH2R7cLV68OOi2TsBYqFAhqVChgmTObHvxAAAALkpKTbDJ0UuyPvPMM3LPPfeY2zVr1pQ///zTTP0WMcGddhwsUqSIdO7cOWj9W2+9Jfv375e+ffvaVjYAAOARljMmMT558qRJdAXS5lltrg2V7VOhTJw4UapUqZJkvaYidUoUAAAAr2jRooXpY/fpp5/K9u3bZd68eWYwRevWrUPehu2ZO52gr1ixpNNKaNOsDqwAAADwyiTGr732mgwYMEC6d+8u+/btM33tunXrJgMHDoyc4E4HTnz33XdStmzZoPW6Tt8QAACAV5plc+fOLWPGjDFLetke3HXt2tWMAjl79qw0adLErFu0aJE8/fTT0rs3l9sCAACIqOBOR4XodfA0/XjmzP9fYitbtmxmIEVa5nQBAABIr1Cv2xoJbJ/E2O/48ePmGmrZs2eXihUrhjxkODlMYuwcTGLsLExiDMDJ7JzEOMedb4Vt2yc/DJ4RxPWZO79cuXLJ1VdfbXcxAACAB1kuytw5Y2gIAAAA3JW5AwAAsI0lrkHmDgAAwEXI3AEAAM+zXNTnjuAOAAB4nuWi4I5mWQAAABchcwcAADzPInMHAAAAJyJzBwAAPM8icwcAAAAnInMHAABgiWuQuQMAAHARMncAAMDzLPrcAQAAwInI3AEAAM+zXJS5I7gDAACeZ7kouKNZFgAAwEXI3AEAAM+zyNwBAADAicjcAQAAWOIaZO4AAABchMwdAADwPIs+dwAAAHAiMncAAMDzLBdl7gjuAACA51kuCu5olgUAAHARMncAAACWuAaZOwAAABchcwcAADzPos8dAAAAnMiVmbu6JfPbXQT8q3LT3nYXAQEOrxpndxEAwJEsMncAAABwIldm7gAAALyauSO4AwAAnme5KLijWRYAAMBFyNwBAABY4hpk7gAAAByiTJkypok48dKjR4+Qt0HmDgAAeJ7lkD53q1atkvPnzyfc/uWXX6RZs2Zy1113hbwNgjsAAACHKFSoUNDt4cOHS/ny5aVx48Yhb4PgDgAAeJ4VxsxdXFycWQJFR0ebJTVnzpyRt99+W3r16pWm8tHnDgAAIIxiY2MlT548QYuuu5D58+fLkSNH5MEHH0zT65G5AwAAnmeFMXPXr18/k30LdKGsnXrzzTfllltukeLFi6fp9QjuAAAArPBtOpQm2MT+/PNP+eqrr2Tu3Llpfj2aZQEAABxm6tSpUrhwYbntttvS/FwydwAAwPMsh0yFouLj401w17FjR8mcOe2hGpk7AAAAB9Hm2B07dkjnzp3T9XwydwAAwPMsB2XumjdvLj6fL93PJ3MHAADgImTuAACA51kOytxdLDJ3AAAALkLmDgAAeJ7loswdwR0AAIAlrkGzLAAAgIuQuQMAAJ5nuahZlswdAACAi5C5AwAAnmeRuQMAAIATkbkDAACeZ7kncUfmDgAAwE3I3AEAAM+zXJS6c1xwd/r0acmWLZvdxQAAAB5iuSe2c0azbHx8vDz//PNSokQJyZUrl/zxxx9m/YABA+TNN9+0u3gAAAARwxHB3bBhw2TatGkyYsQIyZo1a8L6GjVqyJQpU2wtGwAA8EazrBWmxZPB3YwZM2TSpEnSvn17yZQpU8L62rVry6ZNm2wtGwAAQCRxRJ+7Xbt2SYUKFZJtrj179qwtZQIAAN5h0ecuY1WrVk2+/fbbJOvnzJkjderUsaVMAAAAkcgRmbuBAwdKx44dTQZPs3Vz586VzZs3m+baTz75xO7iAQAAl4uKck/qzhGZu5YtW8rHH38sX331leTMmdMEexs3bjTrmjVrZnfxAAAAIoYjMneqUaNG8uWXX9pdDAAA4EGWexJ3zgnuAAAA7GK5KLpzRHCXL1++ZHeqrtOrVehI2gcffFA6depkS/kAAAAihSP63Gkfu6ioKLnttttkyJAhZtG/dV2PHj2kUqVK8sgjj8jkyZPF7X5a+6MM6POo3N2iqTRrUEu+W/K13UXytFw5omVknztl82dD5dDy0bJ4Wi+5slopu4vlWbPfmSW3NGsiV9epKe3vuUt+/uknu4vkWdSFc1AXGcOywrd4MrhbtmyZuUrFzJkzpWfPnmbRv3Xd6tWrTVA3cuRIGTt2rLjd6dOnpFzFytKzd3+7iwIRGT/wPmlSv4p0fm66XNXuRflq+Sb5dEJPKV4oj91F85wFn38mL4+IlW7de8jsD+ZJ5cpV5JFuXeTgwYN2F81zqAvnoC7g2OBu4cKFcuONNyZZ37RpU3OfuvXWWxOuOetmdRs0kk7desq11ze1uyiely06i7Rqerk8O2a+fLfmd/njrwPywsTP5Pe/9kvXuxrZXTzPmTl9qrRp205atb5TyleoIM8NGmK6bcyf+6HdRfMc6sI5qIuMY3H5sYyVP39+M+1JYrpO71MnTpyQ3Llz21A6eFXmTFGSOXMmOX0m+Copp+POSsM65W0rlxedPXNGNv66Qeo3aJiwTrtt1K/fUH5av9bWsnkNdeEc1AUcPaBiwIABpk/d4sWLpW7dumbdqlWr5LPPPpMJEyaY2zpNSuPGjZM8Ny4uzizB60Sio6MvUenhVsdPxsmK9X9Iv663yOZtf8vfB49Ju5uvknq1yprsHS6dw0cOy/nz56VAgQJB6/X2tm3uz+g7CXXhHNRFxrJcNFrWEZm7rl27ypIlS8wExnp1Cl1y5Mhh1nXp0sU8pnfv3vLee+8leW5sbKzkyZMnaHljzAgb3gXcqPNzM0xn2D++eEGOrhwjPe5tLO8v+FHi4312Fw0AAOdm7tQ111xjlrTq16+f9OrVK2jd3ycysGDwtG07D0jzh16VHNmySkyubLL3wDGZObyTbNt1wO6ieUq+vPkkU6ZMSTqJ6+2CBQvaVi4voi6cg7rIWJZ7EnfOyNwpvabsli1bzMjZpUuXBi2p0ebXmJiYoIUmWWS0k6fPmMAub+7scmPDqvLJNz/bXSRPyZI1q1StVl1Wrlge9JmxcuVyqVW7jq1l8xrqwjmoi4xluWhAhSMydytWrJD77rtP/vzzT/H5gpu7dKdonwKvOHXypOzauSPh9t7du2Trlk0SE5NHChctZmvZvOjGBlXNr7kt2/dJ+ZKF5MUnW8mWbX/LjP/+78MUl8YDHTvJgP59pXr1GlKjZi15e+Z0OXXqlLRq3cbuonkOdeEc1AUcG9w9/PDDctVVV8mnn34qxYoVc1WnxrTasmmD9Onx//0M1YSxI82/zW69Q54eMMzGknlTnlzZZGjPO6REkbxy6OhJ+WjROhn0+sdy7ly83UXznJtvuVUOHzokb4wbKwcO7JfKVarKGxOnSAGany456sI5qIuMY7ko9LB8iVNlNtCBFOvXrzeXGcsIOw4Fj56FfSo37W13ERDg8KpxdhcBAFKUzcaU0xVDw3dFqDUDm4jn+tzVq1dPtm7dancxAACAR1n0uctYerkxnepk7969UrNmTcmSJUvQ/bVq1bKtbAAAAJHEEcHdnXfeaf7t3LlzwjqNdLXF2GsDKgAAwKVnuajPnSOCu23bttldBAAAAFdwRHBXunRpu4sAAAA8zHJR6s4Rwd2MGTNSvb9Dhw6XrCwAAACRzBHB3eOPPx50++zZs3Ly5EnJmjWrucYswR0AAAgny0GJu127dknfvn3l888/N/GQThU3depUMydwxAR3hw8fTrLut99+k0ceeUSeeuopW8oEAAC8w3JIdKcx0TXXXCM33HCDCe4KFSpkYqJ8+fKFvA1HBHfJqVixogwfPlzuv/9+2bRpk93FAQAACLuXXnpJSpYsaTJ1fmXLlo28SYxTkjlzZtm9e7fdxQAAAC5nWeFb4uLi5NixY0GLrkvOf//7X9P8etddd0nhwoWlTp06Mnny5DS9F0dk7vSNBNL57fbs2SPjxo0zqUkAAIBIFRsbK0OGDAlaN2jQIBk8eHCSx/7xxx8yfvx46dWrl/Tv319WrVoljz32mBmH0LFjx8i5tmxUVFSSdm9tY27SpImMGjVKihUrlqbtcW1Z5+Dass7CtWUBOJmd15Zt8NLSsG37myfqJcnURUdHmyUxDeI0c/f9998nrNPgToO85cuXR07mLj4+3u4iAAAAhEVKgVxyNKFVrVq1oHVVq1aVDz/8MOTXsy2403RjqEaPHh3WsgAAAG+znDFY1nRH27x5c9C6LVu2pOmCD7YFd2vXro2oockAAADh9uSTT0rDhg3lxRdflHbt2skPP/wgkyZNMovjg7vFixfb9dIAAACOTCZdffXVMm/ePOnXr58MHTrUTIMyZswYad++fcjbcESfOwAAADtZzojtjNtvv90s6eXoee4AAACQNmTuAACA51lOSt1dJDJ3AAAALkLmDgAAeJ5F5g4AAABOROYOAAB4nuWexB2ZOwAAADchcwcAADzPclHqjuAOAAB4nuWe2I5mWQAAADchcwcAADzPclHqjswdAACAi5C5AwAAnme5J3FH5g4AAMBNyNwBAADPi3JR6o7MHQAAgIuQuQMAAJ5nuSdxR3AHAABguSi6o1kWAADARcjcAQAAz4tyT+KOzB0AAICbkLkDAACeZ9HnDgAAAE5E5g4AAHie5Z7EnTuDu8Ix0XYXAf86vGqc3UVAgE827LG7CPhX3ZL57S4C/sV3BtzGlcEdAABAWljintQdwR0AAPC8KPfEdgyoAAAAcBMydwAAwPMsF42oIHMHAADgImTuAACA51nuSdyRuQMAAHATMncAAMDzolyUuiNzBwAA4CJk7gAAgOdZ7kncEdwBAABYLoruaJYFAABwETJ3AADA8yz3JO7I3AEAALgJmTsAAOB5US5K3ZG5AwAAcIjBgwebwR2BS5UqVdK0DTJ3AADA8yxxjurVq8tXX32VcDtz5rSFawR3AAAADqLBXNGiRdP//AwtDQAAQASywtjnLi4uziyBoqOjzZKc3377TYoXLy7ZsmWTBg0aSGxsrJQqVSrk16PPHQAA8LwoK3yLBmd58uQJWnRdcurVqyfTpk2TBQsWyPjx42Xbtm3SqFEj+eeff0J+L5bP5/OJy5w+Z3cJAGf6ZMMeu4uAf9Utmd/uIuBfhWOSz57g0stmY3ti+5nrwrbtt9pVTVPmLtCRI0ekdOnSMnr0aOnSpUtIr0ezLAAA8DwrjM2yoQZyycmbN69UqlRJtm7dGvJzaJYFAABwqOPHj8vvv/8uxYoVC/k5BHcAAMDzLCt8S1r06dNHlixZItu3b5fvv/9eWrduLZkyZZJ777035G3QLAsAAOAQO3fuNIHcwYMHpVChQnLttdfKihUrzN+hIrgDAACeZznk8mOzZ8++6G3QLAsAAOAiZO4AAIDnRTkjceeN4E6n4XNKqhQAALiT5aJYwxHNsiNHjkx2/fnz5+W+++675OUBAACIVI4J7t58880kgd0999wj69aFb8ZoAAAAZYVx8WSz7KeffirNmzc311pr27atnDt3Ttq1ayebNm2SxYsX2108AACAiJGu4O7bb7+ViRMnmhmT58yZIyVKlJCZM2dK2bJlzXwsaXX11VfLhx9+KK1atZKsWbOaLJ5eZkMDuyJFiqSniAAAACGL8nKfOw3CbrrpJsmePbusXbs24UK4R48elRdffDHdBWnSpInMmDFD7rzzTtm2bZuZnZnADgAAIMyZu2HDhsmECROkQ4cOQRPtXXPNNea+ULVp0ybZ9ToDs14k9z//+U/Curlz56a1mAAAACFzUeIu7cHd5s2b5brrrkuyXvvLHTlyJOTt6OOTo1lBAAAAXKLgrmjRoqY/XJkyZYLWL1u2TMqVKxfydqZOnZrWlwYAAAgLy8t97rp27SqPP/64rFy50uyI3bt3y6xZs6RPnz7yyCOPhKeUAAAACE/m7plnnpH4+Hhp2rSpnDx50jTRRkdHm+CuZ8+ekh5///23ef6iRYtk37595qoUiee8AwAACBfL8nBwp9m6Z599Vp566inTPHv8+HGpVq2a5MqVK92FePDBB2XHjh0yYMAAKVasmKtSo+kx+51ZMn3qm3LgwH6pVLmKPNN/gNSsVcvuYnkW9WG/lV98ZJYj+/ea24UvKyM3tO0olevUs7tonvTT2h/lg1nTZMvmjXLowH4ZPHyMXNO4id3F8iw+ozJGlItij3RPYqzz0WlQlxG0v57OnXf55ZeL1y34/DN5eUSsPDdoiNSsWVtmzZwuj3TrIh99skAKFChgd/E8h/pwhpj8heSm+/4jBYpdphecljVLFsqsEc9KjxGTpUjJsnYXz3NOnz4l5SpWlptuby1D+j1pd3E8jc8oZEhwd8MNN6SaWfv666/TukkpWbJkkqZYr5o5faq0adtOWrW+09zWE3bp0m9k/twPpUvX/00Pg0uD+nCGqlc1DLrd/N6H5IcvPpK/fvuV4M4GdRs0Mgvsx2dUxrHck7hL+4AKza7Vrl07YdHs3ZkzZ2TNmjVSs2bNdBVizJgxpi/f9u3bxcvOnjkjG3/dIPUb/O+LLCoqSurXbyg/rV9ra9m8iPpwpvj48/LTd4vkTNxpKVWput3FAWzDZxQyLHP3yiuvJLt+8ODBpv9detx9991mcEb58uUlR44ckiVLlqD7Dx06JF5w+MhhM3gkcSpdb2/b9odt5fIq6sNZ9u74QyY+213OnT0jWbNll/Z9njd97wCv4jMqY1kuSt2lu89dYvfff7/UrVtXXn755XRl7tJLL3/mvwSany9TtBnBC8A9ChYvKY+OnCKnT56QX1YskTmvx0rXIa8S4AFAuIK75cuXS7Zs2dL13I4dO6b7dWNjY2XIkCFB654dMEieGzhYIk2+vPkkU6ZMcvDgwaD1ertgwYK2lcurqA9nyZw5ixQoepn5u0S5yrLr903y/WcfSqv/9La7aIAt+IyyuZ+am4K7xNeE1YEQe/bskR9//NFMZXKxTp8+bfrwBYqJiUnx8f369ZNevXoFlylTZGbtsmTNKlWrVZeVK5ZLk6Y3mnU6p+DKlcvlnnvvt7t4nkN9OJsv3meaaAGv4jMKGRbcJb4mrHberFy5sgwdOlSaN28u6XHixAnp27evvP/++0l+gVxoEmNtfk3cBHv6nESsBzp2kgH9+0r16jWkRs1a8vbM6XLq1Clp1To4qMalQX04w8J3Jkmly+tJ3oKFJe70KVm/7CvZ9us6efDZkXYXzZNOnTwpu3buSLi9d/cu2bplk8TE5JHCRYvZWjav4TMq41he7XOnQVanTp3MqNh8+fJlWCGefvppWbx4sYwfP14eeOABef3112XXrl0yceJEGT58uHjJzbfcKocPHZI3xo01E1JWrlJV3pg4RQqQYrcF9eEMJ44ekTmvvyj/HD4k2XLklKKly5nArkKtq+wumidt2bRB+vToknB7wtj/D7Kb3XqHPD1gmI0l8x4+ozJOlHtiO7F8aZxgTvvVbdy4UcqWzbi5pUqVKiUzZsyQ66+/3jTB6rQqFSpUkJkzZ8q7774rn332WZq2F8mZOyCcPtmwx+4i4F91S+a3uwj4V+GYyOzK40bZMmwkQNo98dEmCZcxLauIo/sP1qhRQ/74I2OHWOtUJ+XKlTN/a3Dnn/rk2muvlaVLl2boawEAACSXuQvXcqmlObgbNmyY9OnTRz755BMzkOLYsWNBS3poYLdt2zbzd5UqVUzfO/Xxxx9L3rx507VNAAAALwo5AaoDJnr37i233nqruX3HHXcEdT7U1l29ndrgh5RoP77169dL48aNzZUqWrRoIePGjZOzZ8/K6NGj07w9AAAArw6oCLnPnc6lo5k67W+XGg3QLtaff/4pq1evNv3uatWqlebn0+cOSB597pyDPnfOQZ8757Czz13vjzeHbdujWlSWSynk3eiPATMieLuQ0qVLmwUAAOBSiHJP4i5tU6FkZMpy7NixIT/2sccey7DXBQAAcLM0BXeVKlW6YIDnH+l6Ia+88kpIj9PXI7gDAADhZHk1c6fXcE18hYr08o+OTan5100dGwEAgLNFuSjuSFNwd88990jhwoXDUpA333zTZPN+++03c7tixYryxBNPyEMPPRSW1wMAAHCjkIO7cGbSBg4caKY86dmzpzRo0MCsW758uTz55JOyY8cOMw0LAACAYyb+dbA0j5YNB72m7OTJk+Xee+9NWKfz6Ok0KBrwEdwBAABkcHAXHx8v4aKTFV91VdILgF955ZVy7hyT1gEAgPCy3NPlzhlZyAceeMBk7xKbNGmStG/f3pYyAQAARCIb54JOOqDiiy++kPr165vbK1euNP3tOnToIL169Up4HJcjAwAAGS3KRak7RwR3v/zyi1xxxRXm799//938W7BgQbPofX5MjwIAABABwd3ixYvtLgIAAPAwy0X5I0f0uQMAALD72rJRYVouxvDhw03Lpc79G/J7ubiXBAAAQDisWrVKJk6caKaGSwuCOwAA4HlRlhW2JT2OHz9uZgzReYDz5cuXtveSrlcEAABASOLi4uTYsWNBi65LTY8ePeS2226TG2+8UdKK4A4AAHieZYVviY2NlTx58gQtui4ls2fPljVr1qT6GMePlgUAAHCrfv36Bc3Zq6Kjo5N97F9//SWPP/64fPnll5ItW7Z0vR7BHQAA8LyoME6FooFcSsFcYqtXr5Z9+/YlzP+rzp8/L0uXLpVx48aZ5txMmTKlug2COwAAAIdo2rSp/Pzzz0HrOnXqJFWqVJG+ffteMLBTBHcAAMDzLHHGLMa5c+eWGjVqBK3LmTOnFChQIMn6lBDcAQAAz4tyRmyXIQjuAAAAHOybb75J0+MJ7gAAgOdFuShzxzx3AAAALkLmDgAAeJ6VzsuEORGZOwAAABchcwcAADwvyj2JOzJ3AAAAbkLmDgAAeJ7loswdwR0AAPC8KBdFdzTLAgAAuAiZOwAA4HlR7knckbkDAABwEzJ3AADA8ywydwAAAHAiMncAAMDzosQ9qTuCO8BDbq9ezO4i4F8VHptvdxHwr++H3WJ3EfCvUvmj7S6CKxDcAQAAz7Pck7gjuAMAAIhyUXDHgAoAAAAXIXMHAAA8L8pF7bJk7gAAAFyEzB0AAPA8yz2JOzJ3AAAAbkLmDgAAeF6Ui1J3ZO4AAABchMwdAADwPMs9iTuCOwAAgChxDze9FwAAAM8jcwcAADzPclG7LJk7AAAAFyFzBwAAPM8S9yBzBwAA4CJk7gAAgOdF0ecOAAAATkTmDgAAeJ4l7kFwBwAAPM9yUXRHsywAAICLkLkDAACeZ7kodUfmDgAAwEXI3AEAAM+LEvdw03sBAADwPDJ3AADA8yz63AEAACCjjR8/XmrVqiUxMTFmadCggXz++edp2gbBHQAA8DwrjEtaXHbZZTJ8+HBZvXq1/Pjjj9KkSRNp2bKlbNiwIeRt0CwLAADgEC1atAi6/cILL5hs3ooVK6R69eohbYPgDgAAeJ4Vxj53cXFxZgkUHR1tltScP39ePvjgAzlx4oRpng0VzbIAAMDzosK4xMbGSp48eYIWXZeSn3/+WXLlymWCv4cffljmzZsn1apVC/m9OCZzt2/fPtm8ebP5u3LlylK4cGG7iwQAAHDR+vXrJ7169Qpal1rWTuOgdevWydGjR2XOnDnSsWNHWbJkScgBnu3B3T///CPdu3eX2bNnm/SjypQpk9x9993y+uuvm+gWAAAgUptlo0Nogg2UNWtWqVChgvn7yiuvlFWrVsmrr74qEydOjIxm2YceekhWrlwpn3zyiRw5csQs+reOEOnWrZvdxQMAALBVfHx8kj57js7caSC3cOFCufbaaxPW3XTTTTJ58mS5+eabbS0bAADwBkuc04R7yy23SKlSpUzr5jvvvCPffPONiZUiJrgrUKBAsk2vui5fvny2lAkAAMCuMQgdOnSQPXv2mFhIJzTWwK5Zs2aRE9w999xzppPhzJkzpWjRombd3r175amnnpIBAwbYXTwAAOABlkNSd2+++eZFb8P24E4n5tu6datJP+qiduzYYToe7t+/P6jz4Jo1a2wsKQAAgPPZHty1atXK7iIAAACPi3JMrzsXBHeDBg2yuwgAAMDjLPfEdvYHd0hq9juzZPrUN+XAgf1SqXIVeab/AKlZq5bdxfIs6sM5qAv7LX++uZQskCPJ+mlL/pDn3vvJljJ52U9rf5QPZk2TLZs3yqED+2Xw8DFyTeMmdhcLNrN9nruoqCgzaXFKi9cs+PwzeXlErHTr3kNmfzBPKleuIo906yIHDx60u2ieRH04B3XhDLe99I3UeebzhOWeV78z6z9ds9vuonnS6dOnpFzFytKzd3+7ixLxrDD+57nMnV4vLdDZs2dl7dq1Mn36dBkyZIh4zczpU6VN23bSqvWd5vZzg4bI0qXfyPy5H0qXrv+xu3ieQ304B3XhDIeOnwm63aN5Udm+77gs/+2AbWXysroNGpkFcFRw17JlyyTr2rZtK9WrV5f33ntPunTpIl5x9swZ2fjrBunStVtQZrN+/Yby0/q1tpbNi6gP56AunClLJkva1L1MJi363e6iABfNclGfO9ubZVNSv359WbRo0QUfp5fjOHbsWNCSlkt0OMnhI4fN9XV1YudAevvAAX4VX2rUh3NQF850U+1iEpM9i3ywYofdRQHg9ODu1KlTMnbsWClRosQFHxsbG2tmcA5cRr4Ue0nKCQBedk/D0rL4133y99HTdhcFyJCpUKLCtHiuWVYvMWYF5EJ9Pp+5llqOHDnk7bffDukabHqFi0C+TNESifLlzWcGkSTuIK63CxYsaFu5vIr6cA7qwnlK5M8ujaoUlq6TVtpdFABOC+5eeeWVoOBO+9EUKlRI6tWrF9K1ZfVKFroEOn1OIlKWrFmlarXqsnLFcmnS9EazLj4+XlauXC733Hu/3cXzHOrDOagL57m7QWk58E+cLPrlb7uLAmQIy0V97mwP7po0aSIlS5YMCvD89DJk/kuSecUDHTvJgP59pXr1GlKjZi15e+Z000zdqnUbu4vmSdSHc1AXzqEf1+3ql5I5K3bI+Xif3cXxtFMnT8qunf/r87h39y7ZumWTxMTkkcJFi9latkhjEdxlnLJly8qePXukcOHCSZpb9D7tRO0lN99yqxw+dEjeGDfWTNRauUpVeWPiFClA05MtqA/noC6co1GVQnJZgRwye/mfdhfF87Zs2iB9evxvVokJY0eaf5vdeoc8PWCYjSWDnSyfdnKzkTbD7t27N0lw9+eff0q1atXkxIkTad5mpDbLAvCOCo/Nt7sI+Nf3w26xuwj4V6n89vWZ/3Jj+EbeN6ta0BuZO/8gCG2OHThwoBlA4afZupUrV8rll19uV/EAAAAikm3BnV6FQmni8Oeff5asWbMm3Kd/165dW/r06WNX8QAAgIdE0efu4i1evNj826lTJ3n11VclJibGrqIAAAC4hu0DKqZOnWp3EQAAgMdZNkw27Krgrk2bNjJt2jSTrdO/UzN37txLVi4AAIBIZ0twp5cI889rpwFecnPcAQAAXCqWi0KRzHY3xY4fP97MNJ8zZ05ze/v27TJ//nypWrWq3HTTTXYUDwAAeIzlombZKLsL0LJlS5k5c6b5+8iRI1K/fn0ZNWqUtGrVygR+AAAAiKDgbs2aNdKoUSPz95w5c6RIkSJmAuMZM2bI2LFj7S4eAADwyFQoUWFaLvl7EZudPHlScufObf7+4osvzAALvWqFZvA0yAMAAEAEBXcVKlQwfez++usvWbhwoTRv3tys37dvH3PfAQCAS9bnzgrTf54L7vTSY3olijJlyki9evWkQYMGCVm8OnXq2F08AACAiGL7JMZt27aVa6+9Vvbs2WMuOebXtGlTad26ta1lAwAA3mC5Z7Cs/cGdKlq0qFkC1a1b17byAAAARCpHBHcAAAB2ssQ9CO4AAIDnRbmoXdb2ARUAAADIOGTuAACA51niHmTuAAAAXITMHQAAgCWuQeYOAADARcjcAQAAz7NclLojcwcAAOAiZO4AAIDnWe5J3BHcAQAAWOIeNMsCAAC4CMEdAACAFcYlDWJjY+Xqq6+W3LlzS+HChaVVq1ayefPmNG2D4A4AAMAhlixZIj169JAVK1bIl19+KWfPnpXmzZvLiRMnQt4Gfe4AAIDnWQ7pdbdgwYKg29OmTTMZvNWrV8t1110X0jYI7gAAAMIoLi7OLIGio6PNciFHjx41/+bPnz/k16NZFgAAeJ5lhW/RfnR58uQJWnTdhcTHx8sTTzwh11xzjdSoUSPk90LmDgAAIIz69esnvXr1CloXStZO+9798ssvsmzZsjS9HsEdAADwPCuM2w61CTbQo48+Kp988oksXbpULrvssjQ9l+AOAADAEkfw+XzSs2dPmTdvnnzzzTdStmzZNG+D4A4AAMAhtCn2nXfekY8++sjMdbd3716zXvvpZc+ePaRtMKACAAB4nhXG/9Ji/PjxZoTs9ddfL8WKFUtY3nvvvZC3QeYOAABAnNMse7EI7gAAgOdZDulzlxFolgUAAHARMncAAMDzLHEPy5cRjbsOc/qc3SWA34adx+wuAgJUvyzG7iIAjnP9y0vsLgL+teKZxra99vod/4Rt27VL5ZZLicwdAACAJa5BcAcAADzPclF0x4AKAAAAFyFzBwAAPM9yT+KOzB0AAICbkLkDAACeZ4l7kLkDAABwETJ3AAAAlrgGmTsAAAAXIXMHAAA8z3JR6o7MHQAAgIuQuQMAAJ5nuSdxR3AHAABgiXvQLAsAAOAiZO4AAAAscQ0ydwAAAC5C5g4AAHie5aLUHZk7AAAAFyFzBwAAPM9yT+KOzB0AAICbkLkDAACeZ4l7ENwBAABY4ho0ywIAALgImTsAAOB5lotSd2TuAAAAXITMHQAA8DzLPYk7MncAAABuQuYOAAB4niXuQeYOAADARWwP7oYOHSonT55Msv7UqVPmPgAAgEuSurPCtHgtuBsyZIgcP348yXoN+PQ+AACASzEVihWm/zwX3Pl8PrGSGaKyfv16yZ8/vy1lAgAAiFS2DajIly+fCep0qVSpUlCAd/78eZPNe/jhh+0qHgAA8BDLRSMqbAvuxowZY7J2nTt3Ns2vefLkSbgva9asUqZMGWnQoIFdxQMAAIhItgV3HTt2NP+WLVtWGjZsKFmyZLGrKAAAwOMscQ/b57lr3Lhxwt+nT5+WM2fOBN0fExNjQ6kAAAAik+0DKnRU7KOPPiqFCxeWnDlzmr54gQsAAICXpkJZunSptGjRQooXL27GJMyfPz+ygrunnnpKvv76axk/frxER0fLlClTTB88fUMzZsywu3gAAACX1IkTJ6R27dry+uuvR2az7Mcff2yCuOuvv146deokjRo1kgoVKkjp0qVl1qxZ0r59e7uLCAAAXM5yUK+7W265xSzpZXtwd+jQISlXrlxC/zq9ra699lp55JFHbC4dAADwAiuMsV1cXJxZAmlrpS7hYHuzrAZ227ZtM39XqVJF3n///YSMXt68ecWLZr8zS25p1kSurlNT2t9zl/z80092F8mTPpo9VZ7r2UE6t2osD7drLqMG95Hdf223u1iexrnhHNSFMxTKlVUG315FFj7eUL7pfa283flKqVI0l93FQiKxsbFmyrfARdeFi+3BnTbF6tUo1DPPPGPal7NlyyZPPvmk6Y/nNQs+/0xeHhEr3br3kNkfzJPKlavII926yMGDB+0umuds/GmNNGtxlwwd85b0ix0n58+fk+H9e8rp06fsLponcW44B3XhDLmjM8ukB+rIuXifPPn+z3LvlB9l7Nd/yD+nz9ldtIhkhXHp16+fHD16NGjRdWF7Lz6dSdhB/vzzT1m9erXpd1erVq10bSOSj2v9BVy9Rk3p/9xAczs+Pl6aN20s9973gHTp+h+JNBt2HhO3OHbksDx8d3MZ8PJEqVrzColE1S+L3KmF3HZuRDK31cX1Ly+RSNS9cVmpdVkeeXjWOnGLFc/8b3q0S+2vQ8HNphmpZP70N7/qaNl58+ZJq1atIqfPXWI6kEIXLzp75oxs/HWDdOnaLWFdVFSU1K/fUH5av9bWskHk5Inj5t9cuSM3QIpUnBvOQV04R6OKBWTFtsPyQqtqUqdkHtl/PE7mrtktH63fa3fRIpLlnPEUF8324G7o0KGp3j9w4P//MvSCw0cOm+vqFihQIGi93t627Q/byoX/z0zMnDBaKlWvLSXLVLC7OJ7DueEc1IVzFM+bXdrUyS7v/rBTpi/fIVWL5pYnb6wgZ8/75LNf/ra7eLgIx48fl61btybc1rEJ69atk/z580upUqWcH9xpqjHQ2bNnzZvInDmzlC9f/oLBXXIjUHyZwjcCBd40ddwI+evP32XQqMl2FwUAjChLZOOef2TC0v8flLjl7+NSvlAOaV2nOMFdujgndffjjz/KDTfckHC7V69eCZdunTZtmvODu7Vrk6bxjx07Jg8++KC0bt36gs/X0SY66XGgZwcMkucGDpZIky9vPsmUKVOSTsl6u2DBgraVy+s0sFu78lsZOGqSFChUxO7ieBLnhnNQF85x4PgZ2X7wZNA6vX195UK2lQkZQ+f+vZghEbaPlk2OznenAduAAQMu+NjkRqA81Td8I1DCKUvWrFK1WnVZuWJ5UHPgypXLpVbtOraWzYv0xNLA7sfvv5FnR4yXwkVL2F0kz+LccA7qwjl+2nlUSuXPEbSuZP4csvfoadvKFOl97qwwLZea7Zm7lPgDtQtJbhLASB4t+0DHTjKgf1+pXr2G1KhZS96eOV1OnTolrVq3sbtonjN13Evy/eKF0nvwy5I9ew45cuiAWZ8jZy7JGp3N7uJ5DueGc1AXzjB71S6Z/MDl0rFBKVm0cZ9UKx4jrWoXk+ELtthdtIhkiXvYHtyNHTs2SbZkz549MnPmzIu69EakuvmWW+XwoUPyxrixcuDAfqlcpaq8MXGKFKC545L76pMPzb/PP/Vw0PpuvQdK4+YtbCqVd3FuOAd14Qwb9/4jfedukEcal5XO15SWPUdOyZhFW2Xhr/vsLhpsZvs8d2XLlg26rUPqCxUqJE2aNDFNrrlz507zNiM5c+c2bprnzg0ieZ47IFwidZ47N7Jznrs9R8+EbdvF8mQVT2Xu/JceAwAAgAuCOwAAALtZLup1Z3twp9Od6KU1QjF37tywlwcAACCS2R7c5cmTx0xkrP9eddVVZp1eW1ZHyup11EIN/AAAANLNReGG7cFdkSJFpF27djJhwgQzMabSS9t0797dzHc3cuRIu4sIAAAQMWyfxPitt96SPn36JAR2Sv/WS23ofQAAAJcicWeFafFccHfu3DnZtGlTkvW6Tmc9BwAACDeLK1RknE6dOkmXLl3k999/l7p165p1K1euNNeM1fsAAAAQQcHdyy+/LEWLFpVRo0aZK1Oo4sWLy9NPPy29e/e2u3gAAMADLBeNqLC9WTYuLk4effRR2bVrlxw5ckTWrVtngrratWsH9cMDAABABAR3LVu2lBkzZpi/tY9d8+bNZfTo0WYalPHjx9tdPAAA4AWWe0ZU2B7crVmzRho1amT+njNnjpka5c8//zQB39ixY+0uHgAAQESxvc/dyZMnJXfu3ObvL774Qtq0aSNRUVFSv359E+QBAACEmyXuYXvmrkKFCjJ//nz566+/ZOHChaZZVu3bt89MYgwAAIAICu4GDhxoJjEuU6aM1KtXTxo0aJCQxatTp47dxQMAAB5gMc9dxmnbtq1ce+21ZhoUHSHr17RpU2ndurWtZQMAAN5guahh1vbgTuk8d7oE8k9oDAAAgAgL7gAAAOxkuSdxZ3+fOwAAAGQcgjsAAAAXIbgDAABwEfrcAQAAz7PocwcAAAAnInMHAAA8z2KeOwAAAPew3BPb0SwLAADgJmTuAACA51niHmTuAAAAXITMHQAAgCWuQeYOAADARcjcAQAAz7NclLojcwcAAOAiZO4AAIDnWe5J3JG5AwAAcBMydwAAwPMscQ+COwAAAEtcg2ZZAAAAFyG4AwAAnmeF8b/0eP3116VMmTKSLVs2qVevnvzwww8hP5fgDgAAwEHee+896dWrlwwaNEjWrFkjtWvXlptuukn27dsX0vMJ7gAAgOdZVviWtBo9erR07dpVOnXqJNWqVZMJEyZIjhw55K233grp+QR3AAAAYRQXFyfHjh0LWnRdcs6cOSOrV6+WG2+8MWFdVFSUub18+XLvjpbN5oJ3pZUeGxsr/fr1k+joaIlUV5aJkUjnlrpwA+rCOdxUFyueaSyRzk314cbYYfCwWBkyZEjQOm1yHTx4cJLHHjhwQM6fPy9FihQJWq+3N23aFNLrWT6fz3eRZUYYaFSfJ08eOXr0qMTERH6AFMmoC+egLpyDunAW6sP5wXdcokydBuHJBeK7d++WEiVKyPfffy8NGjRIWP/000/LkiVLZOXKlRd8PRfkuAAAAJwrOoVALjkFCxaUTJkyyd9//x20Xm8XLVo0pG3Q5w4AAMAhsmbNKldeeaUsWrQoYV18fLy5HZjJSw2ZOwAAAAfRaVA6duwoV111ldStW1fGjBkjJ06cMKNnQ0Fw51CavtXOlnSMtR914RzUhXNQF85CfbjL3XffLfv375eBAwfK3r175fLLL5cFCxYkGWSREgZUAAAAuAh97gAAAFyE4A4AAMBFCO4AAABchODOBR588EFp1aqV3cVwpOuvv16eeOIJu4uBEJQpU8aMCMtInBvpx74DIhfBHQAgiVdffVWmTZuWcJsfSu6l9Zw3b167i4EMxFQoEUyvPWdZlt3FAOBCeikrAJGJzN0lprNMjxgxQipUqGDmIypVqpS88MIL8s0335hA7ciRIwmPXbdunVm3ffv2oF9X//3vf6VatWrm+Tt27LDx3UQW3Zfz588PWqf705+d0P2sj5k7d67ccMMNkiNHDqldu7YsX7486DmTJ0+WkiVLmvtbt24to0ePTvKr96OPPpIrrrhCsmXLJuXKlTMXjD537px4mWZ+Hn30UbNo4KCX2BkwYICkNBuT7teaNWtKzpw5zf7u3r27HD9+POF+//mwcOFCqVq1quTKlUtuvvlm2bNnT4plWLVqlRQqVEheeumlsLzHSDRnzhyzn7Nnzy4FChSQG2+80UyWGtgsq3/rNS01m6fniP9zKbmMj55jgT869cLoOkfXxIkTE86bdu3amWugIvU60B/wOpmt7mNdr9cW1YltA5vLk+vOoPs78IL0qZ1L+t2jE+NqffjrNrmL2SOyENxdYv369ZPhw4ebL7Vff/1V3nnnnZAnJVQnT540X0xTpkyRDRs2SOHChcNaXi969tlnpU+fPia4rlSpktx7770Jgdl3330nDz/8sDz++OPm/mbNmpngPNC3334rHTp0MI/ROtYvNf0STPw4L5o+fbpkzpxZfvjhBxMo6JeOHsvJiYqKkrFjx5rjXJ/39ddfmy+3xOfDyy+/LDNnzpSlS5eaHztad8nR5/vrq2/fvmF5f5FGA2E9vjt37iwbN240X/Rt2rRJEnBrXellj7p27Wqeo4sGCaHaunWrvP/++/Lxxx+biVjXrl1rAgykXgejRo0ynx1vvfWWLFu2TA4dOiTz5s1L82ukdi41bNjQBIcxMTEJdZvSOYQIopMY49I4duyYLzo62jd58uQk9y1evFg/TX2HDx9OWLd27Vqzbtu2beb21KlTze1169YFPbdjx46+li1bXoJ3EHkaN27se/zxx83fuu/mzZsXdH+ePHnMflW6n/UxU6ZMSbh/w4YNZt3GjRvN7bvvvtt32223BW2jffv2Zjt+TZs29b344otBj5k5c6avWLFiPq/XRdWqVX3x8fEJ6/r27WvWqdKlS/teeeWVFJ//wQcf+AoUKJBw238+bN26NWHd66+/7itSpEiSc2Pu3Lm+XLly+WbPnh2Gdxa5Vq9ebfbh9u3bk9yX+HMl8FwKrIPAY1/pORb41TJo0CBfpkyZfDt37kxY9/nnn/uioqJ8e/bs8XldanWgnxkjRoxIuH327FnfZZddFlQvyZ03tWvXNvs9LedS4npEZCNzdwnpr7K4uDhp2rTpRV1QuFatWhlaLgQL3L/FihUz/+7bt8/8u3nzZnOdv0CJb69fv16GDh1qmgn9iz/joZkmL6tfv35Qk51mg3777TfT/JTYV199Zc6VEiVKSO7cueWBBx6QgwcPBu1DbeIrX758UH3568pv5cqVctddd5nsnl7SB/+j3Q50H2uTne4j7XJw+PDhDH8d7X6i9RhY79pFRc8nr0upDrSZVD8z6tWrl/BYzXrrtUbTKpRzCe5CcHcJaX+K1NLmKrA55OzZs8lug0EU6aP7LXFzU3L7OEuWLEHPUfpFFCrty6J97LTZ1r/8/PPPJojRPni4MO3Pdfvtt5tA+8MPP5TVq1fL66+/bu47c+ZMsnWVUh1r8FelShXTtJVcfXtZpkyZ5Msvv5TPP//c9ON97bXXpHLlyrJt27aQnq+fW6GcU0h7Hfj7Wl9sHYR6LsFdCO4uoYoVK5rgbNGiRUnu007eKrAzuAYFyDi6jwP3rwZbaf3lqh+62ik/UOLbOpBCMxI6aCbx4g/ivUqzaIFWrFhhzgv9ggukX0AaUGufI832ad/H3bt3p+s1deCG9jHSfl/akZ/gQ5IExNdcc435QaJ94bR1ILl+Xbo+cYZVz6l//vnHdP5P7XNL+0IG1p/Wu54Lej4h+TrQ7wnNRAeeM9r3V8+N1D7Xjh07FhSch3IuJVe3iGxMhXIJadZGO3JrR1Y9mfRk3r9/v+nkqh3wtYOyjlLSDt9btmwxJyMyTpMmTWTcuHGmSUg/yLQuEmd+LqRnz55y3XXXmYEALVq0MEGD/uIOzKYOHDjQ/FLWpqi2bduaLzFtqv3ll19k2LBh4mX6Ja+j/7p16yZr1qwxWYrkjnMNhDUI0/t1P+tAlgkTJqT7dXXgkdaVjoLWzuuzZ882TVxep4GDBhHNmzc3+0hv62eSjj7+6aefgh6rozL1fs0EaVeD/PnzmyZDbRrv37+/PPbYY+b+wLnxAj/7dJSnDn7R4EMfq4F20aJFxetSqwMdlKUD8PQHkGaf9XMncEYF/+ea7nM9T3RUrX7+BP5YCuVc0rrVFgcthzYTa53qgghmd6c/rzl//rxv2LBhphNslixZfKVKlUrofL9s2TJfzZo1fdmyZfM1atTIdHpNPKAiuU6vDKhIWWAn8F27dvmaN2/uy5kzp69ixYq+zz77LNkBFTqQxU8HuOg6HfDiN2nSJF+JEiV82bNn97Vq1crUZ9GiRYNed8GCBb6GDRuax8TExPjq1q1rnuf1uujevbvv4YcfNvskX758vv79+ycMsEjcMXz06NGmQ7nuw5tuusk3Y8aMoEFHoXTmT3xu7N6921epUiVfu3btfOfOnfN53a+//mr2baFChcxgL903r732WrL7bvPmzb769eub+gj8XNJ9XqFCBbP+9ttvN8d54gEV2sH/jTfe8BUvXtx8vrVt29Z36NAhG95xZNWBDqDQzy89X/Lmzevr1auXr0OHDkH1cvToUTPQSx9TsmRJ37Rp05IMqLjQuaT0vNRBFro+tcEYiAyW/s/uABOIZDpYYtOmTWYKFKQ+z53Ov5XRlxiDs2lrhM59RzeTjKFzDmr2LvGcnUAg2iWANNKmJZ0vTScE1SZZnTfqjTfesLtYAAAYBHdAGukEvHqVEe1Irlef0MlBH3roIbuLBQCAQbMsAACAi3h7XgYAAACXIbgDAABwEYI7AAAAFyG4AwAAcBGCOwAAABchuAPg6AlbW7VqFTQR8hNPPHHJy/HNN9+YS8wlvvQTADgRwR2AdAVdGuzootdJ1utXDh061FzYPJzmzp0rzz//fEiPJSAD4FVMYgwgXW6++WaZOnWqxMXFyWeffSY9evSQLFmySL9+/YIed+bMGRMAZgS9WD0AIHVk7gCkS3R0tBQtWlRKly4tjzzyiNx4443y3//+N6Ep9YUXXpDixYtL5cqVzeP/+usvadeuneTNm9cEaS1btpTt27cnbO/8+fPSq1cvc3+BAgXk6aef1qvPB71m4mZZDSz79u0rJUuWNOXRDOKbb75ptnvDDTeYx+TLl89k8LRcKj4+XmJjY6Vs2bKSPXt2qV27tsyZMyfodTRYrVSpkrlftxNYTgBwOoI7ABlCAyHN0qlFixbJ5s2b5csvv5RPPvlEzp49KzfddJPkzp1bvv32W/nuu+8kV65cJvvnf86oUaNk2rRp8tZbb8myZcvk0KFDMm/evFRfs0OHDvLuu++aS8Bt3LhRJk6caLarwd6HH35oHqPl2LNnj7z66qvmtgZ2M2bMkAkTJsiGDRvkySeflPvvv1+WLFmSEIS2adNGWrRoYS52r5eWe+aZZ8K89wAg49AsC+CiaHZNg7mFCxdKz549Zf/+/ZIzZ06ZMmVKQnPs22+/bTJmuk6zaEqbdDVLp33jmjdvLmPGjDFNuhpYKQ2+dJsp2bJli7z//vsmgNSsodJr/SZuwi1cuLB5HX+m78UXX5SvvvpKGjRokPAcDSY1MGzcuLGMHz9eypcvb4JNpZnHn3/+WV566aUw7UEAyFgEdwDSRTNymiXTrJwGbvfdd58MHjzY9L2rWbNmUD+79evXy9atW03mLtDp06fl999/l6NHj5rsWr169RLuy5w5s1x11VVJmmb9NKuWKVMmE5CFSstw8uRJadasWdB6zR7WqVPH/K0ZwMByKH8gCACRgOAOQLpoXzTNcmkQp33rNBjz08xdoOPHj8uVV14ps2bNSrKdQoUKpbsZOK20HOrTTz+VEiVKBN2nffYAwA0I7gCkiwZwOoAhFFdccYW89957pok0JiYm2ccUK1ZMVq5cKdddd525rdOqrF692jw3OZod1Iyh9pXzN8sG8mcOdaCGX7Vq1UwQt2PHjhQzflWrVjUDQwKtWLEipPcJAE7AgAoAYde+fXspWLCgGSGrAyq2bdtm+to99thjsnPnTvOYxx9/XIYPHy7z58+XTZs2Sffu3VOdo65MmTLSsWNH6dy5s3mOf5vaD0/pKF7t36fNx9oPULN22izcp08fM4hi+vTppkl4zZo18tprr5nb6uGHH5bffvtNnnrqKTMY45133jEDPQAgUhDcAQi7HDlyyNKlS6VUqVJmwIRmx7p06WL63Pkzeb1795YHHnjABGzax00DsdatW6e6XW0Wbtu2rQkEq1SpIl27dpUTJ06Y+7TZdciQIWaka5EiReTRRx8163US5AEDBphRs1oOHbGrzbQ6NYrSMupIWw0YdZoUHdihgzAAIFJYvpR6KwMAACDikLkDAABwEYI7AAAAFyG4AwAAcBGCOwAAABchuAMAAHARgjsAAAAXIbgDAABwEYI7AAAAFyG4AwAAcBGCOwAAABchuAMAABD3+D//ZjhrOP+moQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    ytrue = np.argmax(y_test, axis=1)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    \n",
    "    # Create a combined confusion matrix\n",
    "    cm = confusion_matrix(ytrue, yhat)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=actions, yticklabels=actions)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abc22d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Attention_128HUs classification accuracy = 82.353%\n"
     ]
    }
   ],
   "source": [
    "# Collect results \n",
    "eval_results['confusion matrix'] = confusion_matrices\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Model accuracy\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ba92f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Attention_128HUs weighted average precision = 0.834\n",
      "LSTM_Attention_128HUs weighted average recall = 0.824\n",
      "LSTM_Attention_128HUs weighted average f1-score = 0.824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect results \n",
    "eval_results['accuracy'] = classification_accuracies\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Get unique labels for proper classification report\n",
    "    unique_labels = np.unique(np.concatenate([ytrue, yhat]))\n",
    "    present_actions = [actions[i] for i in unique_labels]\n",
    "    \n",
    "    # Precision, recall, and f1 score\n",
    "    try:\n",
    "        report = classification_report(ytrue, yhat, labels=unique_labels, target_names=present_actions, output_dict=True)\n",
    "        \n",
    "        precisions[model_name] = report['weighted avg']['precision']\n",
    "        recalls[model_name] = report['weighted avg']['recall']\n",
    "        f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "       \n",
    "        print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "        print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "        print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate detailed metrics for {model_name}: {e}\")\n",
    "        # Use basic accuracy as fallback\n",
    "        precisions[model_name] = classification_accuracies[model_name]\n",
    "        recalls[model_name] = classification_accuracies[model_name]\n",
    "        f1_scores[model_name] = classification_accuracies[model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da013cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores\n",
    "\n",
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    \"\"\"\n",
    "    Computes 3D joint angle inferred by 3 keypoints and their relative positions to one another\n",
    "    \"\"\"\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41906417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(landmarks, mp_pose, side, joint):\n",
    "    \"\"\"\n",
    "    Retrieves x and y coordinates of a particular keypoint from the pose estimation model\n",
    "         \n",
    "     Args:\n",
    "         landmarks: processed keypoints from the pose estimation model\n",
    "         mp_pose: Mediapipe pose estimation model\n",
    "         side: 'left' or 'right'. Denotes the side of the body of the landmark of interest.\n",
    "         joint: 'shoulder', 'elbow', 'wrist', 'hip', 'knee', or 'ankle'. Denotes which body joint is associated with the landmark of interest.\n",
    "    \"\"\"\n",
    "    coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "    x_coord_val = landmarks[coord.value].x\n",
    "    y_coord_val = landmarks[coord.value].y\n",
    "    return [x_coord_val, y_coord_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_joint_angle(image, angle, joint):\n",
    "    \"\"\"\n",
    "    Displays the joint angle value near the joint within the image frame\n",
    "    \"\"\"\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                   tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Counts repetitions of each exercise. Global count and stage (i.e., state) variables are updated within this function.\n",
    "    Updated for: curl, push up, and plank exercises\n",
    "    \"\"\"\n",
    "\n",
    "    global curl_counter, squat_counter, plank_counter, curl_stage, squat_stage, plank_stage, plank_start_time\n",
    "    \n",
    "    if current_action == 'curl':\n",
    "        # Get coords for bicep curl\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        \n",
    "        # Calculate elbow angle\n",
    "        angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        # Curl counter logic\n",
    "        if angle < 30:\n",
    "            curl_stage = \"up\" \n",
    "        if angle > 140 and curl_stage =='up':\n",
    "            curl_stage=\"down\"  \n",
    "            curl_counter +=1\n",
    "        \n",
    "        # Reset other stages\n",
    "        pushup_stage = None\n",
    "        plank_stage = None\n",
    "            \n",
    "        # Viz joint angle\n",
    "        viz_joint_angle(image, angle, elbow)\n",
    "        \n",
    "    elif current_action == 'squat':\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        \n",
    "        right_hip = get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        \n",
    "        avg_knee_angle = (left_knee_angle + right_knee_angle) / 2\n",
    "        \n",
    "        if avg_knee_angle < 90:\n",
    "            squat_stage = \"down\"\n",
    "        if avg_knee_angle > 160 and squat_stage == 'down':\n",
    "            squat_stage = \"up\"\n",
    "            squat_counter += 1\n",
    "        \n",
    "        curl_stage = None\n",
    "        plank_stage = None\n",
    "        \n",
    "        viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "        viz_joint_angle(image, right_knee_angle, right_knee)\n",
    "     \n",
    "    elif current_action == 'plank':\n",
    "        # Get coords for plank\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        \n",
    "        # Calculate body alignment angles\n",
    "        shoulder_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        hip_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        \n",
    "        # Plank detection logic - check if body is straight and parallel to ground\n",
    "        # Good plank: shoulder-hip-knee should be close to 180 degrees (straight line)\n",
    "        body_straight = shoulder_hip_angle > 160 and hip_knee_angle > 160\n",
    "        \n",
    "        # Check if person is in horizontal position (plank position)\n",
    "        # Compare shoulder and hip y-coordinates (should be roughly similar)\n",
    "        horizontal_position = abs(left_shoulder[1] - left_hip[1]) < 0.1\n",
    "        \n",
    "        if body_straight and horizontal_position:\n",
    "            if plank_stage != \"holding\":\n",
    "                plank_stage = \"holding\"\n",
    "                plank_start_time = time.time()  # Start timing\n",
    "        else:\n",
    "            if plank_stage == \"holding\":\n",
    "                # Calculate hold duration\n",
    "                hold_duration = time.time() - plank_start_time\n",
    "                if hold_duration > 3:  # Minimum 3 seconds to count as a plank\n",
    "                    plank_counter += 1\n",
    "                plank_stage = \"not_holding\"\n",
    "        \n",
    "        # Reset other stages\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "        \n",
    "        # Viz joint angles\n",
    "        viz_joint_angle(image, shoulder_hip_angle, left_hip)\n",
    "        viz_joint_angle(image, hip_knee_angle, left_knee)\n",
    "        \n",
    "        # Display plank timer if holding\n",
    "        if plank_stage == \"holding\":\n",
    "            current_hold_time = time.time() - plank_start_time\n",
    "            cv2.putText(image, f'Hold: {current_hold_time:.1f}s', (10, 400), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    \"\"\"\n",
    "    This function displays the model prediction probability distribution over the set of exercise classes\n",
    "    as a horizontal bar graph\n",
    "    \"\"\"\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc22e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "predictions = []\n",
    "res = []\n",
    "threshold = 0.5 # minimum confidence to classify as an action/exercise\n",
    "current_action = ''\n",
    "\n",
    "# Rep counter logic variables - updated for your exercises\n",
    "curl_counter = 0\n",
    "squat_counter = 0\n",
    "plank_counter = 0\n",
    "curl_stage = None\n",
    "squat_stage = None\n",
    "plank_stage = None\n",
    "plank_start_time = 0\n",
    "\n",
    "# Camera object\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video writer object that saves a video of the real time test\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') # video compression format\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # webcam video frame height\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # webcam video frame width\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS)) # webcam video fram rate \n",
    "\n",
    "video_name = os.path.join(os.getcwd(),f\"{model_name}_real_time_test.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH,HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(res))\n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "        #3. Viz logic\n",
    "            # Erase current action variable if no probability is above threshold\n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "            # Count reps\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                count_reps(image, current_action, landmarks, mp_pose)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Display graphical information - updated for your exercises\n",
    "            cv2.rectangle(image, (0,0), (640, 40), colors[np.argmax(res)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(curl_counter), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'pushup ' + str(pushup_counter), (200,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'plank ' + str(plank_counter), (420,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "         \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        # Write to video file\n",
    "        if ret == True:\n",
    "            out.write(image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_model(input_video_path, model_path, actions, output_video_path=None):\n",
    "    \"\"\"\n",
    "    Process a video file with the trained exercise recognition model\n",
    "    \n",
    "    Args:\n",
    "        input_video_path: Path to input video file\n",
    "        model_path: Path to trained model (.h5 file)\n",
    "        actions: List of exercise action names\n",
    "        output_video_path: Path to save output video (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the trained model\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Model parameters (should match training parameters)\n",
    "    sequence_length = 30\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Colors for visualization (should match training colors)\n",
    "    colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "    while len(colors) < len(actions):\n",
    "        colors.append((np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255)))\n",
    "    \n",
    "    # Initialize detection variables\n",
    "    sequence = []\n",
    "    predictions = []\n",
    "    res = []\n",
    "    current_action = ''\n",
    "    \n",
    "    # Initialize rep counters\n",
    "    global curl_counter, squat_counter, plank_counter, curl_stage, squat_stage, plank_stage, plank_start_time\n",
    "    curl_counter = 0\n",
    "    squat_counter = 0\n",
    "    plank_counter = 0\n",
    "    curl_stage = None\n",
    "    squat_stage = None\n",
    "    plank_stage = None\n",
    "    plank_start_time = 0\n",
    "    \n",
    "    # Open input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    # Setup output video writer if output path is provided\n",
    "    out = None\n",
    "    if output_video_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        print(f\"Output video will be saved to: {output_video_path}\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    # Process video with MediaPipe pose detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')\n",
    "            \n",
    "            # Make detection\n",
    "            image, results = mediapipe_detection(frame, pose)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_landmarks(image, results)\n",
    "            \n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-sequence_length:]\n",
    "            if len(sequence) == sequence_length:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "                predictions.append(np.argmax(res))\n",
    "                \n",
    "                # FIX: Add bounds checking for action index\n",
    "                predicted_index = np.argmax(res)\n",
    "                if predicted_index < len(actions):\n",
    "                    current_action = actions[predicted_index]\n",
    "                else:\n",
    "                    print(f\"Warning: Model predicted class {predicted_index} but only {len(actions)} actions available\")\n",
    "                    current_action = 'unknown'\n",
    "                \n",
    "                confidence = np.max(res)\n",
    "                \n",
    "                # Erase current action variable if no probability is above threshold\n",
    "                if confidence < threshold:\n",
    "                    current_action = ''\n",
    "                \n",
    "                # Visualize probabilities (only for valid actions)\n",
    "                if predicted_index < len(actions):\n",
    "                    image = prob_viz(res[:len(actions)], actions, image, colors[:len(actions)])\n",
    "                \n",
    "                # Count reps\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    count_reps(image, current_action, landmarks, mp_pose)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Display graphical information\n",
    "                if predicted_index < len(actions):\n",
    "                    cv2.rectangle(image, (0,0), (width, 40), colors[predicted_index], -1)\n",
    "                else:\n",
    "                    cv2.rectangle(image, (0,0), (width, 40), (128, 128, 128), -1)  # Gray for unknown\n",
    "                \n",
    "                cv2.putText(image, f'Curl: {curl_counter}', (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'Squat: {squat_counter}', (200, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'Plank: {plank_counter}', (420, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Display current action and confidence\n",
    "                cv2.putText(image, f'Action: {current_action} ({confidence:.2f})', (10, height-20), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Write frame to output video if specified\n",
    "            if out is not None:\n",
    "                out.write(image)\n",
    "            \n",
    "            # Display frame (optional - comment out for faster processing)\n",
    "            cv2.imshow('Exercise Recognition', image)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Final counts:\")\n",
    "    print(f\"  Curls: {curl_counter}\")\n",
    "    print(f\"  Squats: {squat_counter}\")\n",
    "    print(f\"  Planks: {plank_counter}\")\n",
    "    \n",
    "    if output_video_path:\n",
    "        print(f\"Output video saved to: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration - UPDATE THESE PATHS\n",
    "    INPUT_VIDEO_PATH = r\"C:\\Users\\pheno\\Downloads\\work\\main_model\\2nd test vid.mp4\"  # Change this to your input video path\n",
    "    MODEL_PATH = \"LSTM_Attention_128HUs.h5\"  # Path to your trained model\n",
    "    OUTPUT_VIDEO_PATH = \"output_exercise_recognition.mp4\"  # Where to save the output video\n",
    "    \n",
    "    # Actions (should match your training data)\n",
    "    # Update this list to match the actions you trained your model on\n",
    "    actions = np.array(['curl','squat','plank'])  # Modify based on your training data\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(INPUT_VIDEO_PATH):\n",
    "        print(f\"Error: Input video file not found: {INPUT_VIDEO_PATH}\")\n",
    "        print(\"Please update INPUT_VIDEO_PATH with the correct path to your video file.\")\n",
    "    elif not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model file not found: {MODEL_PATH}\")\n",
    "        print(\"Please update MODEL_PATH with the correct path to your trained model.\")\n",
    "    else:\n",
    "        # Process the video\n",
    "        process_video_with_model(\n",
    "            input_video_path=INPUT_VIDEO_PATH,\n",
    "            model_path=MODEL_PATH,\n",
    "            actions=actions,\n",
    "            output_video_path=OUTPUT_VIDEO_PATH\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
